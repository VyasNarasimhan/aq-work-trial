{
    "id": "c99e9ff1-6ddb-4e66-90ae-404021a2d10f",
    "results": [
        {
            "id": "2cc2fd0c-3186-48db-bc41-500340834285",
            "trial_name": "broken-postgres-backup-restore.1-of-1.2025-12-04__08-58-41",
            "task_id": "broken-postgres-backup-restore",
            "instruction": "## Background\n\nA production PostgreSQL database backup system has failed, and you've been tasked with recovering the database from backup files. The backup files are partially corrupted and need to be cleaned before importing.\n\n## Your Task\n\nThe backup files are located in `/backup/`. Your goal is to restore the database, clean corrupted data, and generate required reports.\n\n## Available Files\n\nYou have access to the following files in `/backup/`:\n\n- `database.sql` - SQL dump file with database schema\n- `users_export.csv` - User data export with columns: user_id, username, email, created_at, status (may have malformed rows)\n- `transactions_backup.json` - Transaction records (may contain null values, invalid amounts, or corrupted entries)\n- `backup_metadata.log` - Backup process log file containing information about the backup\n\n## Requirements\n\nYou must complete the following tasks and produce outputs in `/backup/`:\n\n### 1. Database Restoration\n\n- Create a database named `production_db` that is accessible\n- The database must contain tables named `users` and `transactions`\n- Both tables must be created and ready to accept data\n\n### 2. Data Cleaning and Import (`clean_users.csv`)\n\n- Clean the `users_export.csv` file by removing invalid rows\n- Remove rows with: missing user_ids, invalid email formats (must contain @), empty usernames, or invalid status values\n- Valid status values are: \"active\", \"inactive\", \"suspended\"\n- Import the cleaned data into a table named `users` in the `production_db` database\n- Output the cleaned data to `/backup/clean_users.csv`\n- Format: CSV with header line, newline-terminated\n- Columns: user_id,username,email,created_at,status\n\n### 3. Transaction Recovery (`valid_transactions.json`)\n\n- Process `transactions_backup.json` to extract only valid transactions\n- Remove transactions with: null transaction_id, null or negative amounts, missing timestamps, or invalid user references\n- Valid user_id values must be in the range 1-30 (matching the valid users in the database)\n- Transaction amounts must be positive numbers\n- Import valid transactions into a table named `transactions` in the `production_db` database\n- Output valid transactions to `/backup/valid_transactions.json`\n- Format: Valid JSON array, newline-terminated\n- Each transaction must have: transaction_id, user_id, amount, timestamp, description\n\n### 4. Database Statistics Report (`db_stats.txt`)\n\n- Query the restored database to generate statistics\n- Calculate and report the following metrics:\n  - Total number of users in the database\n  - Number of active users (status = 'active')\n  - Total number of transactions\n  - Sum of all transaction amounts (2 decimal places)\n  - Average transaction amount per user (2 decimal places)\n- Output format: Plain text with clear labels, newline-terminated\n- Example structure:\n  ```\n  DATABASE STATISTICS\n  ==================\n  Total Users: <count>\n  Active Users: <count>\n  Total Transactions: <count>\n  Total Amount: <amount>\n  Average per User: <amount>\n  ```\n\n### 5. Top Users Report (`top_users.csv`)\n\n- Identify the top 10 users by total transaction amount\n- Query the database to join users and transactions tables\n- Calculate total amount per user and sort descending\n- Output format: CSV file with header, newline-terminated\n- Columns: username,total_amount\n- Sort by total_amount descending, then by username ascending (for ties)\n- Include exactly 10 rows (excluding header)\n- Amounts must have 2 decimal places (e.g., 1234.56)\n\n### 6. Backup Verification (`backup_verification.txt`)\n\n- Analyze the `backup_metadata.log` file to extract backup information\n- Report the following details exactly as found in the log:\n  - Original backup timestamp (found on line with \"Backup timestamp:\")\n  - Number of tables backed up (found on line with \"Total tables backed up:\")\n  - Total rows in original backup (found on line with \"Total rows backed up:\")\n  - Data integrity status (found on line with \"Integrity check:\")\n- Output format: Plain text with clear labels, newline-terminated\n- Example structure:\n  ```\n  BACKUP VERIFICATION\n  ===================\n  Backup Date: 2025-09-28 03:00:00\n  Tables Backed Up: 2\n  Total Rows: 130\n  Integrity Check: PASSED\n  ```\n\n### 7. Recovery Completion Marker\n\n- After successfully completing all tasks, create a file at `/backup/recovery_complete.txt`\n- Content: Single line with text \"RECOVERY_SUCCESSFUL\" followed by newline\n- This file indicates that all restoration and analysis tasks are complete\n\n## Tools Available\n\n- `postgresql` (service commands: `service postgresql start/stop/status`)\n- `psql` - PostgreSQL command-line client\n- `python3` - Data processing and analysis\n- `jq` - JSON processing\n- Standard Unix tools: `grep`, `awk`, `sed`, `cut`, `sort`, etc.\n\n## Tips\n\n- CSV files may have rows with missing fields (represented by consecutive commas)\n- Use Python's csv and json modules for robust data cleaning\n- PostgreSQL tools like psql can be used for database operations\n- Remember to handle edge cases: NULL values, empty strings, and malformed data\n- All numeric outputs should be properly formatted with specified decimal places\n- All output files must be newline-terminated\n\n## Validation\n\nAll outputs will be validated for:\n- Correct file locations and names in `/backup`\n- Proper file formatting (newline termination, valid CSV/JSON structure)\n- Successful database restoration and data import\n- Accurate data cleaning and filtering\n- Correct calculations and query results\n- Complete recovery process with verification",
            "is_resolved": true,
            "failure_mode": "unset",
            "parser_results": {
                "TestDatabaseRestoration::test_database_exists_and_accessible": "passed",
                "TestDataCleaning::test_clean_users_csv_format_and_content": "passed",
                "TestDataCleaning::test_valid_transactions_json_format_and_content": "passed",
                "TestDatabaseReports::test_database_statistics_report": "passed",
                "TestDatabaseReports::test_top_users_report": "passed",
                "TestBackupVerification::test_backup_verification_report": "passed",
                "TestRecoveryCompletion::test_recovery_complete_marker": "passed"
            },
            "recording_path": "2025-12-04__08-58-41/broken-postgres-backup-restore/broken-postgres-backup-restore.1-of-1.2025-12-04__08-58-41/sessions/agent.cast",
            "total_input_tokens": 113215,
            "total_output_tokens": 5264,
            "trial_started_at": "2025-12-04T16:58:41.192443+00:00",
            "trial_ended_at": "2025-12-04T17:03:09.503766+00:00",
            "agent_started_at": "2025-12-04T17:00:07.908185+00:00",
            "agent_ended_at": "2025-12-04T17:02:22.813471+00:00",
            "test_started_at": "2025-12-04T17:02:26.095105+00:00",
            "test_ended_at": "2025-12-04T17:03:07.104097+00:00"
        }
    ],
    "pass_at_k": {},
    "n_resolved": 1,
    "n_unresolved": 0,
    "resolved_ids": [
        "broken-postgres-backup-restore"
    ],
    "unresolved_ids": [],
    "accuracy": 1.0
}