['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
'psql -U postgres -c "CREATE DATABASE production_db;"\n'
'psql -U postgres production_db < /backup/database.sql\n'
'psql -U postgres production_db -c "\\dt"\n'
'psql -U postgres production_db -c "\\d users"\n'
'psql -U postgres production_db -c "\\d transactions"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql production_db < /backup/database.sql"\n'
'su - postgres -c "psql production_db -c \'\\dt\'"\n'
'su - postgres -c "psql production_db -c \'\\d users\'"\n'
'su - postgres -c "psql production_db -c \'\\d transactions\'"\n'
"cat > /backup/clean_users.py << 'EOF'\nimport csv\nimport re\n\n# Valid status values\nvalid_statuses = {'active', 'inactive', 'suspended'}\n\n# Email regex pattern\nemail_pattern = re.compile(r'.+@.+')\n\nwith open('/backup/users_export.csv', 'r') as infile, open('/backup/clean_users.csv', 'w', newline='') as outfile:\n    reader = csv.DictReader(infile)\n    writer = csv.DictWriter(outfile, fieldnames=['user_id', 'username', 'email', 'created_at', 'status'])\n    writer.writeheader()\n    \n    for row in reader:\n        # Check if all required fields exist and are not empty\n        try:\n            user_id = row.get('user_id', '').strip()\n            username = row.get('username', '').strip()\n            email = row.get('email', '').strip()\n            status = row.get('status', '').strip()\n            created_at = row.get('created_at', '').strip()\n            \n            # Validate row\n            if not user_id:  # Missing user_id\n                continue\n            if not username:  # Empty username\n                continue\n            if not email or not email_pattern.match(email):  # Invalid email\n                continue\n            if status not in valid_statuses:  # Invalid status\n                continue\n            \n            # Write valid row\n            writer.writerow({\n                'user_id': user_id,\n                'username': username,\n                'email': email,\n                'created_at': created_at,\n                'status': status\n            })\n        except Exception as e:\n            # Skip malformed rows\n            continue\nEOF\n"
'python3 /backup/clean_users.py\n'
'head -10 /backup/clean_users.csv\n'
'wc -l /backup/clean_users.csv\n'
"cat > /backup/clean_transactions.py << 'EOF'\nimport json\n\nwith open('/backup/transactions_backup.json', 'r') as infile:\n    transactions = json.load(infile)\n\nvalid_transactions = []\n\nfor txn in transactions:\n    try:\n        # Get transaction fields\n        txn_id = txn.get('transaction_id')\n        user_id = txn.get('user_id')\n        amount = txn.get('amount')\n        timestamp = txn.get('timestamp')\n        description = txn.get('description')\n        \n        # Validate transaction\n        if txn_id is None:  # Null transaction_id\n            continue\n        if amount is None or amount <= 0:  # Null or negative amount\n            continue\n        if not timestamp:  # Missing timestamp\n            continue\n        if user_id is None or user_id < 1 or user_id > 30:  # Invalid user reference\n            continue\n        \n        # Add valid transaction\n        valid_transactions.append({\n            'transaction_id': txn_id,\n            'user_id': user_id,\n            'amount': amount,\n            'timestamp': timestamp,\n            'description': description\n        })\n    except Exception as e:\n        # Skip malformed transactions\n        continue\n\n# Write valid transactions to output file\nwith open('/backup/valid_transactions.json', 'w') as outfile:\n    json.dump(valid_transactions, outfile, indent=2)\n    outfile.write('\\n')\nEOF\n"
'python3 /backup/clean_transactions.py\n'
'head -30 /backup/valid_transactions.json\n'
"jq 'length' /backup/valid_transactions.json\n"
'cat > /backup/import_data.py << \'EOF\'\nimport csv\nimport json\nimport subprocess\n\n# Import users using PostgreSQL COPY command\nprint("Importing users...")\nresult = subprocess.run([\n    \'su\', \'-\', \'postgres\', \'-c\',\n    "psql production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f"Error: {result.stderr}")\n\n# Import transactions from JSON\nprint("\\nImporting transactions...")\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Create SQL file for transactions\nwith open(\'/tmp/transactions.sql\', \'w\') as f:\n    for txn in transactions:\n        f.write(f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{txn[\'description\'].replace(\\"\'\\", \\"\'\'\\")}\');\\n")\n\nresult = subprocess.run([\n    \'su\', \'-\', \'postgres\', \'-c\',\n    "psql production_db < /tmp/transactions.sql"\n], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f"Error: {result.stderr}")\n\nprint("\\nData import complete!")\nEOF\n'
'python3 /backup/import_data.py\n'
'cat > /backup/import_data.py << \'EOF\'\nimport csv\nimport json\nimport subprocess\n\n# Import users using PostgreSQL COPY command\nprint("Importing users...")\nresult = subprocess.run([\n    \'su\', \'-\', \'postgres\', \'-c\',\n    "psql production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f"Error: {result.stderr}")\n\n# Import transactions from JSON\nprint("\\nImporting transactions...")\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Create SQL file for transactions\nwith open(\'/tmp/transactions.sql\', \'w\') as f:\n    for txn in transactions:\n        desc = txn[\'description\'].replace("\'", "\'\'")\n        sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\');\\n"\n        f.write(sql)\n\nresult = subprocess.run([\n    \'su\', \'-\', \'postgres\', \'-c\',\n    "psql production_db < /tmp/transactions.sql"\n], capture_output=True, text=True)\nprint(result.stdout)\nif result.returncode != 0:\n    print(f"Error: {result.stderr}")\n\nprint("\\nData import complete!")\nEOF\n'
'python3 /backup/import_data.py\n'
'su - postgres -c "psql production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /backup/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\n# Generate Database Statistics Report\nprint("Generating database statistics report...")\n\n# Query for statistics\nqueries = {\n    \'total_users\': "SELECT COUNT(*) FROM users;",\n    \'active_users\': "SELECT COUNT(*) FROM users WHERE status = \'active\';",\n    \'total_transactions\': "SELECT COUNT(*) FROM transactions;",\n    \'total_amount\': "SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;",\n    \'avg_per_user\': "SELECT ROUND(AVG(total)::numeric, 2) FROM (SELECT SUM(amount) as total FROM transactions GROUP BY user_id) as user_totals;"\n}\n\nstats = {}\nfor key, query in queries.items():\n    result = subprocess.run([\n        \'su\', \'-\', \'postgres\', \'-c\',\n        f\'psql production_db -t -c "{query}"\'\n    ], capture_output=True, text=True)\n    stats[key] = result.stdout.strip()\n\n# Write db_stats.txt\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {stats["total_users"]}\\n\')\n    f.write(f\'Active Users: {stats["active_users"]}\\n\')\n    f.write(f\'Total Transactions: {stats["total_transactions"]}\\n\')\n    f.write(f\'Total Amount: {stats["total_amount"]}\\n\')\n    f.write(f\'Average per User: {stats["avg_per_user"]}\\n\')\n\nprint("Database statistics report generated.")\n\n# Generate Top Users Report\nprint("\\nGenerating top users report...")\n\ntop_users_query = """\nSELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n"""\n\nresult = subprocess.run([\n    \'su\', \'-\', \'postgres\', \'-c\',\n    f\'psql production_db -c "{top_users_query}" --csv\'\n], capture_output=True, text=True)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(result.stdout)\n\nprint("Top users report generated.")\n\n# Generate Backup Verification Report\nprint("\\nGenerating backup verification report...")\n\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\n# Extract information from log\nbackup_date = \'\'\ntables_backed_up = \'\'\ntotal_rows = \'\'\nintegrity_check = \'\'\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write(\'BACKUP VERIFICATION\\n\')\n    f.write(\'===================\\n\')\n    f.write(f\'Backup Date: {backup_date}\\n\')\n    f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n    f.write(f\'Total Rows: {total_rows}\\n\')\n    f.write(f\'Integrity Check: {integrity_check}\\n\')\n\nprint("Backup verification report generated.")\n\n# Create recovery completion marker\nprint("\\nCreating recovery completion marker...")\nwith open(\'/backup/recovery_complete.txt\', \'w\') as f:\n    f.write(\'RECOVERY_SUCCESSFUL\\n\')\n\nprint("\\nAll reports generated successfully!")\nEOF\n'
'python3 /backup/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'cat /backup/backup_verification.txt\n'
'cat /backup/recovery_complete.txt\n'
'ls -la /backup/*.txt /backup/*.csv /backup/*.json\n'
'su - postgres -c "psql production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as txn_count FROM transactions;\'"\n'
'wc -l /backup/clean_users.csv /backup/top_users.csv /backup/valid_transactions.json\n'
'tail -c 20 /backup/db_stats.txt | od -c\n'
'tail -c 20 /backup/top_users.csv | od -c\n'
'tail -c 20 /backup/backup_verification.txt | od -c\n'
'tail -c 20 /backup/recovery_complete.txt | od -c\n'
"echo '=== FINAL VERIFICATION ==='\n"
'echo \'Database Status:\' && su - postgres -c "psql -l | grep production_db"\n'
'echo \'Tables:\' && su - postgres -c "psql production_db -c \'\\dt\'"\n'
"echo 'Generated Files:' && ls -1 /backup/*.txt /backup/clean_users.csv /backup/top_users.csv /backup/valid_transactions.json 2>/dev/null | xargs -n1 basename\n"
"echo 'All tasks completed successfully!'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
