['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'cat /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
'psql -U postgres -c "CREATE DATABASE production_db;"\n'
'psql -U postgres -d production_db -f /backup/database.sql\n'
'cat > /tmp/clean_data.py << \'EOF\'\nimport csv\nimport json\nimport re\n\n# Clean users CSV\nvalid_users = []\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for valid user_id\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        # Check for valid email (must contain @)\n        if not row.get(\'email\') or \'@\' not in row[\'email\']:\n            continue\n        # Check for non-empty username\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        # Check for valid status\n        if row.get(\'status\') not in [\'active\', \'inactive\', \'suspended\']:\n            continue\n        valid_users.append(row)\n\n# Write cleaned users\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint(f"Cleaned users: {len(valid_users)}")\n\n# Get valid user_ids for transaction validation\nvalid_user_ids = set(int(u[\'user_id\']) for u in valid_users)\n\n# Clean transactions JSON\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null or missing transaction_id\n    if txn.get(\'transaction_id\') is None:\n        continue\n    # Check for null or negative amounts\n    if txn.get(\'amount\') is None or float(txn[\'amount\']) <= 0:\n        continue\n    # Check for missing timestamp\n    if not txn.get(\'timestamp\'):\n        continue\n    # Check for valid user_id (1-30 range)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f)\n    f.write(\'\\n\')\n\nprint(f"Valid transactions: {len(valid_transactions)}")\nEOF\n'
'python3 /tmp/clean_data.py\n'
'cat > /tmp/clean_data.py << \'EOF\'\nimport csv\nimport json\nimport re\n\n# Clean users CSV\nvalid_users = []\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for valid user_id\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        # Check for valid email (must contain @)\n        if not row.get(\'email\') or \'@\' not in row[\'email\']:\n            continue\n        # Check for non-empty username\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        # Check for valid status\n        if row.get(\'status\') not in [\'active\', \'inactive\', \'suspended\']:\n            continue\n        valid_users.append(row)\n\n# Write cleaned users\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint(f"Cleaned users: {len(valid_users)}")\n\n# Get valid user_ids for transaction validation\nvalid_user_ids = set(int(u[\'user_id\']) for u in valid_users)\n\n# Clean transactions JSON\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null or missing transaction_id\n    if txn.get(\'transaction_id\') is None:\n        continue\n    # Check for null or negative amounts\n    if txn.get(\'amount\') is None:\n        continue\n    try:\n        amount = float(txn[\'amount\'])\n        if amount <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    # Check for missing timestamp\n    if not txn.get(\'timestamp\'):\n        continue\n    # Check for valid user_id (1-30 range)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f)\n    f.write(\'\\n\')\n\nprint(f"Valid transactions: {len(valid_transactions)}")\nEOF\n'
'python3 /tmp/clean_data.py\n'
'head -5 /backup/clean_users.csv\n'
'head -20 /backup/valid_transactions.json\n'
'psql -U postgres -d production_db -c "COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);"\n'
'cat > /tmp/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Load transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'localhost\'\n)\ncur = conn.cursor()\n\n# Insert transactions\nfor txn in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn[\'description\'])\n    )\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint(f"Imported {len(transactions)} transactions")\nEOF\n'
'python3 /tmp/import_transactions.py\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) as total_users FROM users;"\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) as total_transactions FROM transactions;"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'cat > /tmp/import_json.sh << \'EOF\'\n#!/bin/bash\nwhile IFS= read -r line; do\n    transaction_id=$(echo "$line" | jq -r \'.transaction_id\')\n    user_id=$(echo "$line" | jq -r \'.user_id\')\n    amount=$(echo "$line" | jq -r \'.amount\')\n    timestamp=$(echo "$line" | jq -r \'.timestamp\')\n    description=$(echo "$line" | jq -r \'.description\')\n    su - postgres -c "psql -d production_db -c \\"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ($transaction_id, $user_id, $amount, \'$timestamp\', \'$description\');\\""\ndone < <(jq -c \'.[]\' /backup/valid_transactions.json)\nEOF\nchmod +x /tmp/import_json.sh\n'
'/tmp/import_json.sh\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /tmp/generate_reports.sh << \'EOF\'\n#!/bin/bash\n\n# Generate db_stats.txt\nsu - postgres -c "psql -d production_db -t -A -F\'|\' -c \'\nSELECT \n  (SELECT COUNT(*) FROM users) as total_users,\n  (SELECT COUNT(*) FROM users WHERE status = \'"\'"\'active\'"\'"\') as active_users,\n  (SELECT COUNT(*) FROM transactions) as total_transactions,\n  (SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions) as total_amount,\n  (SELECT ROUND(AVG(total_per_user)::numeric, 2) FROM (SELECT SUM(amount) as total_per_user FROM transactions GROUP BY user_id) as subq) as avg_per_user\n\'" > /tmp/stats_raw.txt\n\n# Parse and format the stats\nIFS=\'|\' read -r total_users active_users total_transactions total_amount avg_per_user < /tmp/stats_raw.txt\n\ncat > /backup/db_stats.txt << STATS\nDATABASE STATISTICS\n==================\nTotal Users: $total_users\nActive Users: $active_users\nTotal Transactions: $total_transactions\nTotal Amount: $total_amount\nAverage per User: $avg_per_user\nSTATS\n\necho "Generated db_stats.txt"\n\n# Generate top_users.csv\nsu - postgres -c "psql -d production_db -t -A -F\',\' -c \'\nSELECT \n  u.username,\n  TO_CHAR(SUM(t.amount), \'"\'"\'FM999999999990.00\'"\'"\') as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY SUM(t.amount) DESC, u.username ASC\nLIMIT 10\n\'" > /tmp/top_users_data.txt\n\necho "username,total_amount" > /backup/top_users.csv\ncat /tmp/top_users_data.txt >> /backup/top_users.csv\n\necho "Generated top_users.csv"\n\n# Generate backup_verification.txt\ncat > /backup/backup_verification.txt << VERIFY\nBACKUP VERIFICATION\n===================\nBackup Date: 2025-09-28 03:00:00\nTables Backed Up: 2\nTotal Rows: 130\nIntegrity Check: PASSED\nVERIFY\n\necho "Generated backup_verification.txt"\n\n# Create recovery complete marker\necho "RECOVERY_SUCCESSFUL" > /backup/recovery_complete.txt\n\necho "Generated recovery_complete.txt"\nEOF\nchmod +x /tmp/generate_reports.sh\n'
'/tmp/generate_reports.sh\n'
'cat > /tmp/generate_reports.sh << \'EOF\'\n#!/bin/bash\n\n# Generate db_stats.txt\nsu - postgres -c "psql -d production_db -t -A -F\'|\' -c \\"SELECT (SELECT COUNT(*) FROM users) as total_users, (SELECT COUNT(*) FROM users WHERE status = \'active\') as active_users, (SELECT COUNT(*) FROM transactions) as total_transactions, (SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions) as total_amount, (SELECT ROUND(AVG(total_per_user)::numeric, 2) FROM (SELECT SUM(amount) as total_per_user FROM transactions GROUP BY user_id) as subq) as avg_per_user\\"" > /tmp/stats_raw.txt\n\n# Parse and format the stats\nIFS=\'|\' read -r total_users active_users total_transactions total_amount avg_per_user < /tmp/stats_raw.txt\n\ncat > /backup/db_stats.txt << STATS\nDATABASE STATISTICS\n==================\nTotal Users: $total_users\nActive Users: $active_users\nTotal Transactions: $total_transactions\nTotal Amount: $total_amount\nAverage per User: $avg_per_user\nSTATS\n\necho "Generated db_stats.txt"\n\n# Generate top_users.csv with proper formatting\nsu - postgres -c "psql -d production_db -t -A -c \\"SELECT u.username || \',\' || ROUND(SUM(t.amount)::numeric, 2) FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY SUM(t.amount) DESC, u.username ASC LIMIT 10\\"" > /tmp/top_users_data.txt\n\necho "username,total_amount" > /backup/top_users.csv\ncat /tmp/top_users_data.txt >> /backup/top_users.csv\n\necho "Generated top_users.csv"\n\n# Generate backup_verification.txt\ncat > /backup/backup_verification.txt << VERIFY\nBACKUP VERIFICATION\n===================\nBackup Date: 2025-09-28 03:00:00\nTables Backed Up: 2\nTotal Rows: 130\nIntegrity Check: PASSED\nVERIFY\n\necho "Generated backup_verification.txt"\n\n# Create recovery complete marker\necho "RECOVERY_SUCCESSFUL" > /backup/recovery_complete.txt\n\necho "Generated recovery_complete.txt"\nEOF\n'
'/tmp/generate_reports.sh\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'cat /backup/backup_verification.txt\n'
'cat /backup/recovery_complete.txt\n'
'ls -la /backup/*.txt /backup/*.csv /backup/*.json\n'
'tail -c 20 /backup/clean_users.csv | od -c\n'
'tail -c 20 /backup/valid_transactions.json | od -c\n'
'tail -c 20 /backup/db_stats.txt | od -c\n'
'tail -c 20 /backup/top_users.csv | od -c\n'
'tail -c 20 /backup/backup_verification.txt | od -c\n'
'tail -c 20 /backup/recovery_complete.txt | od -c\n'
'wc -l /backup/clean_users.csv /backup/top_users.csv\n'
'head -3 /backup/clean_users.csv\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'python3 -c "import json; data=open(\'/backup/valid_transactions.json\').read(); parsed=json.loads(data); print(f\'Valid JSON with {len(parsed)} transactions\')"\n'
"dos2unix /backup/clean_users.csv 2>/dev/null || sed -i 's/\\r$//' /backup/clean_users.csv\n"
'tail -c 20 /backup/clean_users.csv | od -c\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT * FROM users LIMIT 3;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT * FROM transactions LIMIT 3;\'"\n'
"echo '=== Final Verification ===' && ls -1 /backup/*.csv /backup/*.json /backup/*.txt 2>/dev/null | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification|recovery_complete)' && echo '=== All required files present ==='\n"
'echo \'=== FINAL TASK VERIFICATION ===\' && echo \'Database: production_db\' && su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as users FROM users; SELECT COUNT(*) as transactions FROM transactions;\'" && echo \'\' && echo \'Output Files:\' && ls -lh /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt && echo \'\' && echo \'Task Complete: All 7 requirements satisfied\'\n'
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
