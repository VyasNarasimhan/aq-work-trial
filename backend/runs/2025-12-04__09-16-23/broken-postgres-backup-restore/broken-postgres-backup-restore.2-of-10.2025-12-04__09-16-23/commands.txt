['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
'service postgresql start\n'
'wc -l /backup/users_export.csv /backup/transactions_backup.json\n'
'cat /backup/users_export.csv\n'
'cat /backup/transactions_backup.json\n'
'cat /backup/database.sql\n'
'psql -U postgres -c "CREATE DATABASE production_db;"\n'
'psql -U postgres -d production_db -f /backup/database.sql\n'
'psql -U postgres -d production_db -c "\\dt"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
"cat > /backup/clean_users.py << 'EOF'\nimport csv\nimport re\n\n# Valid status values\nvalid_statuses = {'active', 'inactive', 'suspended'}\n\n# Email regex pattern\nemail_pattern = re.compile(r'^[^@]+@[^@]+$')\n\nwith open('/backup/users_export.csv', 'r') as infile, open('/backup/clean_users.csv', 'w', newline='') as outfile:\n    reader = csv.DictReader(infile)\n    writer = csv.DictWriter(outfile, fieldnames=['user_id', 'username', 'email', 'created_at', 'status'])\n    writer.writeheader()\n    \n    for row in reader:\n        # Check if user_id is missing or empty\n        if not row.get('user_id') or row['user_id'].strip() == '':\n            continue\n        \n        # Check if username is empty\n        if not row.get('username') or row['username'].strip() == '':\n            continue\n        \n        # Check if email is valid (must contain @)\n        if not row.get('email') or not email_pattern.match(row['email']):\n            continue\n        \n        # Check if status is valid\n        if not row.get('status') or row['status'] not in valid_statuses:\n            continue\n        \n        # Write valid row\n        writer.writerow(row)\nEOF\n"
'python3 /backup/clean_users.py\n'
'wc -l /backup/clean_users.csv\n'
'head -5 /backup/clean_users.csv\n'
'su - postgres -c "psql -d production_db -c \\"COPY users FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
"cat > /backup/clean_transactions.py << 'EOF'\nimport json\n\n# Read the transactions file\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor trans in transactions:\n    # Check if transaction_id is null or missing\n    if trans.get('transaction_id') is None:\n        continue\n    \n    # Check if user_id is valid (1-30)\n    user_id = trans.get('user_id')\n    if user_id is None or user_id < 1 or user_id > 30:\n        continue\n    \n    # Check if amount is valid (positive number)\n    amount = trans.get('amount')\n    if amount is None:\n        continue\n    \n    # Check if amount is a valid number and positive\n    try:\n        amount_float = float(amount)\n        if amount_float <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check if timestamp is not null\n    if trans.get('timestamp') is None:\n        continue\n    \n    # Add to valid transactions\n    valid_transactions.append(trans)\n\n# Write to output file\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\nEOF\n"
'python3 /backup/clean_transactions.py\n'
'head -20 /backup/valid_transactions.json\n'
'tail -20 /backup/valid_transactions.json\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Insert transactions\nfor trans in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (trans[\'transaction_id\'], trans[\'user_id\'], trans[\'amount\'], trans[\'timestamp\'], trans[\'description\'])\n    )\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint(f"Imported {len(transactions)} transactions")\nEOF\n'
'python3 /backup/import_transactions.py\n'
'cat > /backup/json_to_csv.py << \'EOF\'\nimport json\nimport csv\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Write to CSV\nwith open(\'/backup/transactions.csv\', \'w\', newline=\'\') as f:\n    writer = csv.DictWriter(f, fieldnames=[\'transaction_id\', \'user_id\', \'amount\', \'timestamp\', \'description\'])\n    writer.writeheader()\n    for trans in transactions:\n        writer.writerow(trans)\n\nprint(f"Converted {len(transactions)} transactions to CSV")\nEOF\n'
'python3 /backup/json_to_csv.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY transactions FROM \'/backup/transactions.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
