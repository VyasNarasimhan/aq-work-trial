['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'cat database.sql\n'
'head -20 users_export.csv\n'
'head -30 transactions_backup.json\n'
'cat database.sql\n'
'cat backup_metadata.log\n'
'wc -l users_export.csv transactions_backup.json\n'
'psql -U postgres -c "CREATE DATABASE production_db;"\n'
'psql -U postgres -d production_db -f database.sql\n'
'psql -U postgres -d production_db -c "\\dt"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'cat > clean_and_import.py << \'EOF\'\nimport csv\nimport json\nimport re\nimport subprocess\n\n# Clean users CSV\nprint("Cleaning users CSV...")\nvalid_users = []\nvalid_statuses = {\'active\', \'inactive\', \'suspended\'}\nemail_pattern = re.compile(r\'^[^@]+@[^@]+$\')\n\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for missing user_id\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        \n        # Check for empty username\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        \n        # Check for invalid email (must contain @)\n        if not row.get(\'email\') or not email_pattern.match(row[\'email\'].strip()):\n            continue\n        \n        # Check for invalid status\n        if not row.get(\'status\') or row[\'status\'].strip() not in valid_statuses:\n            continue\n        \n        valid_users.append(row)\n\nprint(f"Valid users: {len(valid_users)}")\n\n# Write clean users CSV\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        fieldnames = [\'user_id\', \'username\', \'email\', \'created_at\', \'status\']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint("Clean users CSV written.")\n\n# Clean transactions JSON\nprint("Cleaning transactions JSON...")\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_user_ids = {int(u[\'user_id\']) for u in valid_users}\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get(\'transaction_id\') is None:\n        continue\n    \n    # Check for null or negative amount\n    if txn.get(\'amount\') is None or txn[\'amount\'] <= 0:\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get(\'timestamp\'):\n        continue\n    \n    # Check for invalid user_id (must be 1-30 and in valid users)\n    user_id = txn.get(\'user_id\')\n    if user_id is None or user_id < 1 or user_id > 30:\n        continue\n    \n    valid_transactions.append(txn)\n\nprint(f"Valid transactions: {len(valid_transactions)}")\n\n# Write valid transactions JSON\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint("Valid transactions JSON written.")\n\n# Import users into database\nprint("Importing users into database...")\nwith open(\'/tmp/import_users.sql\', \'w\') as f:\n    f.write("COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\n")\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -f /tmp/import_users.sql"],\n    capture_output=True, text=True\n)\nprint(result.stdout)\nif result.stderr:\n    print(result.stderr)\n\n# Import transactions into database\nprint("Importing transactions into database...")\nwith open(\'/tmp/import_transactions.sql\', \'w\') as f:\n    for txn in valid_transactions:\n        f.write(f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{txn[\'description\']}\');\\n")\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -f /tmp/import_transactions.sql"],\n    capture_output=True, text=True\n)\nprint(result.stdout)\nif result.stderr:\n    print(result.stderr)\n\nprint("Data import complete.")\nEOF\n'
'python3 clean_and_import.py\n'
'cat > clean_and_import.py << \'EOF\'\nimport csv\nimport json\nimport re\nimport subprocess\n\n# Clean users CSV\nprint("Cleaning users CSV...")\nvalid_users = []\nvalid_statuses = {\'active\', \'inactive\', \'suspended\'}\nemail_pattern = re.compile(r\'^[^@]+@[^@]+$\')\n\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for missing user_id\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        \n        # Check for empty username\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        \n        # Check for invalid email (must contain @)\n        if not row.get(\'email\') or not email_pattern.match(row[\'email\'].strip()):\n            continue\n        \n        # Check for invalid status\n        if not row.get(\'status\') or row[\'status\'].strip() not in valid_statuses:\n            continue\n        \n        valid_users.append(row)\n\nprint(f"Valid users: {len(valid_users)}")\n\n# Write clean users CSV\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        fieldnames = [\'user_id\', \'username\', \'email\', \'created_at\', \'status\']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint("Clean users CSV written.")\n\n# Clean transactions JSON\nprint("Cleaning transactions JSON...")\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_user_ids = {int(u[\'user_id\']) for u in valid_users}\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get(\'transaction_id\') is None:\n        continue\n    \n    # Check for null or negative amount\n    amount = txn.get(\'amount\')\n    if amount is None:\n        continue\n    \n    try:\n        amount_val = float(amount)\n        if amount_val <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get(\'timestamp\'):\n        continue\n    \n    # Check for invalid user_id (must be 1-30 and in valid users)\n    user_id = txn.get(\'user_id\')\n    if user_id is None:\n        continue\n    \n    try:\n        user_id_val = int(user_id)\n        if user_id_val < 1 or user_id_val > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(txn)\n\nprint(f"Valid transactions: {len(valid_transactions)}")\n\n# Write valid transactions JSON\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint("Valid transactions JSON written.")\n\n# Import users into database\nprint("Importing users into database...")\nwith open(\'/tmp/import_users.sql\', \'w\') as f:\n    f.write("COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\n")\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -f /tmp/import_users.sql"],\n    capture_output=True, text=True\n)\nprint(result.stdout)\nif result.stderr:\n    print(result.stderr)\n\n# Import transactions into database\nprint("Importing transactions into database...")\nwith open(\'/tmp/import_transactions.sql\', \'w\') as f:\n    for txn in valid_transactions:\n        desc = txn[\'description\'].replace("\'", "\'\'")\n        f.write(f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\');\\n")\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -f /tmp/import_transactions.sql"],\n    capture_output=True, text=True\n)\nprint(result.stdout)\nif result.stderr:\n    print(result.stderr)\n\nprint("Data import complete.")\nEOF\n'
'python3 clean_and_import.py\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > generate_reports.py << \'EOF\'\nimport subprocess\nimport re\n\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f"psql -d production_db -t -A -c \\"{query}\\""],\n        capture_output=True, text=True\n    )\n    return result.stdout.strip()\n\n# Generate database statistics report\nprint("Generating database statistics report...")\ntotal_users = run_query("SELECT COUNT(*) FROM users;")\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query("SELECT COUNT(*) FROM transactions;")\ntotal_amount = run_query("SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;")\navg_per_user = run_query("SELECT ROUND(AVG(total)::numeric, 2) FROM (SELECT user_id, SUM(amount) as total FROM transactions GROUP BY user_id) as user_totals;")\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write("DATABASE STATISTICS\\n")\n    f.write("==================\\n")\n    f.write(f"Total Users: {total_users}\\n")\n    f.write(f"Active Users: {active_users}\\n")\n    f.write(f"Total Transactions: {total_transactions}\\n")\n    f.write(f"Total Amount: {total_amount}\\n")\n    f.write(f"Average per User: {avg_per_user}\\n")\n\nprint("Database statistics report generated.")\n\n# Generate top users report\nprint("Generating top users report...")\nquery = """\nSELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n"""\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -F"," -c "{query}"\'],\n    capture_output=True, text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write("username,total_amount\\n")\n    f.write(result.stdout)\n\nprint("Top users report generated.")\n\n# Generate backup verification report\nprint("Generating backup verification report...")\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\n# Extract information from log\nbackup_date = None\ntables_backed_up = None\ntotal_rows = None\nintegrity_check = None\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write("BACKUP VERIFICATION\\n")\n    f.write("===================\\n")\n    f.write(f"Backup Date: {backup_date}\\n")\n    f.write(f"Tables Backed Up: {tables_backed_up}\\n")\n    f.write(f"Total Rows: {total_rows}\\n")\n    f.write(f"Integrity Check: {integrity_check}\\n")\n\nprint("Backup verification report generated.")\n\n# Create recovery completion marker\nprint("Creating recovery completion marker...")\nwith open(\'/backup/recovery_complete.txt\', \'w\') as f:\n    f.write("RECOVERY_SUCCESSFUL\\n")\n\nprint("Recovery completion marker created.")\nprint("All reports generated successfully!")\nEOF\n'
'python3 generate_reports.py\n'
'ls -la /backup/*.csv /backup/*.json /backup/*.txt 2>/dev/null\n'
'echo "=== clean_users.csv ==="  && head -5 /backup/clean_users.csv\n'
'echo "=== db_stats.txt ===" && cat /backup/db_stats.txt\n'
'echo "=== top_users.csv ===" && cat /backup/top_users.csv\n'
'echo "=== backup_verification.txt ===" && cat /backup/backup_verification.txt\n'
'echo "=== recovery_complete.txt ===" && cat /backup/recovery_complete.txt\n'
'echo "=== valid_transactions.json (first 20 lines) ===" && head -20 /backup/valid_transactions.json\n'
'tail -5 /backup/valid_transactions.json\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as txn_count FROM transactions;\'"\n'
'wc -l /backup/clean_users.csv /backup/top_users.csv /backup/db_stats.txt /backup/backup_verification.txt /backup/recovery_complete.txt /backup/valid_transactions.json\n'
'echo "=== Verifying all files end with newline ===" && for f in /backup/clean_users.csv /backup/top_users.csv /backup/db_stats.txt /backup/backup_verification.txt /backup/recovery_complete.txt /backup/valid_transactions.json; do echo -n "$f: "; tail -c 1 "$f" | od -An -tx1; done\n'
'echo "=== Verifying top_users.csv has exactly 10 data rows ===" && tail -n +2 /backup/top_users.csv | wc -l\n'
'echo "=== Verifying clean_users.csv has 30 data rows ===" && tail -n +2 /backup/clean_users.csv | wc -l\n'
'echo "=== Final validation: All required files present ===" && ls -1 /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt\n'
'echo "=== TASK COMPLETION SUMMARY ===" && echo "Database: production_db with users and transactions tables" && echo "Users imported: 30" && echo "Transactions imported: 100" && echo "All required files generated and validated"\n'
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
