['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'cat /backup/backup_metadata.log\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat > /backup/restore_database.py << \'EOF\'\nimport psycopg2\nimport csv\nimport json\nimport re\nfrom decimal import Decimal\n\n# Create database and restore schema\ndef create_database():\n    # Connect to default postgres database\n    conn = psycopg2.connect(dbname=\'postgres\', user=\'postgres\', host=\'localhost\')\n    conn.autocommit = True\n    cur = conn.cursor()\n    \n    # Drop if exists and create new database\n    cur.execute("DROP DATABASE IF EXISTS production_db")\n    cur.execute("CREATE DATABASE production_db")\n    cur.close()\n    conn.close()\n    \n    # Connect to new database and create tables\n    conn = psycopg2.connect(dbname=\'production_db\', user=\'postgres\', host=\'localhost\')\n    cur = conn.cursor()\n    \n    # Create users table\n    cur.execute("""\n        CREATE TABLE users (\n            user_id INTEGER PRIMARY KEY,\n            username VARCHAR(255) NOT NULL,\n            email VARCHAR(255) NOT NULL,\n            created_at TIMESTAMP,\n            status VARCHAR(50)\n        )\n    """)\n    \n    # Create transactions table\n    cur.execute("""\n        CREATE TABLE transactions (\n            transaction_id INTEGER PRIMARY KEY,\n            user_id INTEGER,\n            amount DECIMAL(10, 2),\n            timestamp TIMESTAMP,\n            description TEXT\n        )\n    """)\n    \n    conn.commit()\n    cur.close()\n    conn.close()\n\n# Clean users data\ndef clean_users():\n    valid_statuses = [\'active\', \'inactive\', \'suspended\']\n    email_pattern = re.compile(r\'^[^@]+@[^@]+$\')\n    \n    clean_data = []\n    with open(\'/backup/users_export.csv\', \'r\') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Check for missing user_id\n            if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n                continue\n            \n            # Check for empty username\n            if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n                continue\n            \n            # Check for invalid email\n            email = row.get(\'email\', \'\').strip()\n            if not email or not email_pattern.match(email):\n                continue\n            \n            # Check for invalid status\n            status = row.get(\'status\', \'\').strip().lower()\n            if status not in valid_statuses:\n                continue\n            \n            clean_data.append({\n                \'user_id\': row[\'user_id\'].strip(),\n                \'username\': row[\'username\'].strip(),\n                \'email\': email,\n                \'created_at\': row.get(\'created_at\', \'\').strip(),\n                \'status\': status\n            })\n    \n    # Write cleaned data\n    with open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(clean_data)\n    \n    return clean_data\n\n# Clean transactions data\ndef clean_transactions():\n    with open(\'/backup/transactions_backup.json\', \'r\') as f:\n        transactions = json.load(f)\n    \n    valid_transactions = []\n    for txn in transactions:\n        # Check for null transaction_id\n        if txn.get(\'transaction_id\') is None:\n            continue\n        \n        # Check for null or negative amounts\n        amount = txn.get(\'amount\')\n        if amount is None or amount <= 0:\n            continue\n        \n        # Check for missing timestamp\n        if not txn.get(\'timestamp\'):\n            continue\n        \n        # Check for valid user_id (1-30)\n        user_id = txn.get(\'user_id\')\n        if user_id is None or user_id < 1 or user_id > 30:\n            continue\n        \n        valid_transactions.append(txn)\n    \n    # Write valid transactions\n    with open(\'/backup/valid_transactions.json\', \'w\') as f:\n        json.dump(valid_transactions, f, indent=2)\n        f.write(\'\\n\')\n    \n    return valid_transactions\n\n# Import data into database\ndef import_data(users, transactions):\n    conn = psycopg2.connect(dbname=\'production_db\', user=\'postgres\', host=\'localhost\')\n    cur = conn.cursor()\n    \n    # Import users\n    for user in users:\n        cur.execute("""\n            INSERT INTO users (user_id, username, email, created_at, status)\n            VALUES (%s, %s, %s, %s, %s)\n        """, (user[\'user_id\'], user[\'username\'], user[\'email\'], user[\'created_at\'], user[\'status\']))\n    \n    # Import transactions\n    for txn in transactions:\n        cur.execute("""\n            INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description)\n            VALUES (%s, %s, %s, %s, %s)\n        """, (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn.get(\'description\', \'\')))\n    \n    conn.commit()\n    cur.close()\n    conn.close()\n\n# Generate database statistics\ndef generate_stats():\n    conn = psycopg2.connect(dbname=\'production_db\', user=\'postgres\', host=\'localhost\')\n    cur = conn.cursor()\n    \n    # Total users\n    cur.execute("SELECT COUNT(*) FROM users")\n    total_users = cur.fetchone()[0]\n    \n    # Active users\n    cur.execute("SELECT COUNT(*) FROM users WHERE status = \'active\'")\n    active_users = cur.fetchone()[0]\n    \n    # Total transactions\n    cur.execute("SELECT COUNT(*) FROM transactions")\n    total_transactions = cur.fetchone()[0]\n    \n    # Total amount\n    cur.execute("SELECT COALESCE(SUM(amount), 0) FROM transactions")\n    total_amount = float(cur.fetchone()[0])\n    \n    # Average per user\n    if total_users > 0:\n        avg_per_user = total_amount / total_users\n    else:\n        avg_per_user = 0\n    \n    cur.close()\n    conn.close()\n    \n    # Write stats\n    with open(\'/backup/db_stats.txt\', \'w\') as f:\n        f.write(\'DATABASE STATISTICS\\n\')\n        f.write(\'==================\\n\')\n        f.write(f\'Total Users: {total_users}\\n\')\n        f.write(f\'Active Users: {active_users}\\n\')\n        f.write(f\'Total Transactions: {total_transactions}\\n\')\n        f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n        f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\n# Generate top users report\ndef generate_top_users():\n    conn = psycopg2.connect(dbname=\'production_db\', user=\'postgres\', host=\'localhost\')\n    cur = conn.cursor()\n    \n    cur.execute("""\n        SELECT u.username, COALESCE(SUM(t.amount), 0) as total_amount\n        FROM users u\n        LEFT JOIN transactions t ON u.user_id = t.user_id\n        GROUP BY u.username\n        ORDER BY total_amount DESC, u.username ASC\n        LIMIT 10\n    """)\n    \n    results = cur.fetchall()\n    cur.close()\n    conn.close()\n    \n    # Write top users\n    with open(\'/backup/top_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.writer(f)\n        writer.writerow([\'username\', \'total_amount\'])\n        for username, total_amount in results:\n            writer.writerow([username, f\'{float(total_amount):.2f}\'])\n\n# Generate backup verification\ndef generate_verification():\n    with open(\'/backup/backup_metadata.log\', \'r\') as f:\n        content = f.read()\n    \n    backup_date = \'\'\n    tables_backed_up = \'\'\n    total_rows = \'\'\n    integrity_check = \'\'\n    \n    for line in content.split(\'\\n\'):\n        if \'Backup timestamp:\' in line:\n            backup_date = line.split(\'Backup timestamp:\')[1].strip()\n        elif \'Total tables backed up:\' in line:\n            tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n        elif \'Total rows backed up:\' in line:\n            total_rows = line.split(\'Total rows backed up:\')[1].strip()\n        elif \'Integrity check:\' in line:\n            integrity_check = line.split(\'Integrity check:\')[1].strip()\n    \n    with open(\'/backup/backup_verification.txt\', \'w\') as f:\n        f.write(\'BACKUP VERIFICATION\\n\')\n        f.write(\'===================\\n\')\n        f.write(f\'Backup Date: {backup_date}\\n\')\n        f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n        f.write(f\'Total Rows: {total_rows}\\n\')\n        f.write(f\'Integrity Check: {integrity_check}\\n\')\n\n# Main execution\nif __name__ == \'__main__\':\n    print(\'Creating database...\')\n    create_database()\n    \n    print(\'Cleaning users data...\')\n    users = clean_users()\n    \n    print(\'Cleaning transactions data...\')\n    transactions = clean_transactions()\n    \n    print(\'Importing data...\')\n    import_data(users, transactions)\n    \n    print(\'Generating statistics...\')\n    generate_stats()\n    \n    print(\'Generating top users report...\')\n    generate_top_users()\n    \n    print(\'Generating backup verification...\')\n    generate_verification()\n    \n    print(\'Creating recovery completion marker...\')\n    with open(\'/backup/recovery_complete.txt\', \'w\') as f:\n        f.write(\'RECOVERY_SUCCESSFUL\\n\')\n    \n    print(\'Recovery complete!\')\nEOF\n'
'python3 /backup/restore_database.py\n'
'pip3 install psycopg2-binary\n'
'python3 /backup/restore_database.py\n'
'cat > /backup/restore_database.py << \'EOF\'\nimport subprocess\nimport csv\nimport json\nimport re\nimport os\n\n# Create database and restore schema\ndef create_database():\n    # Drop and create database\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-c\', \'DROP DATABASE IF EXISTS production_db\'], \n                   env={**os.environ, \'PGHOST\': \'localhost\'})\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-c\', \'CREATE DATABASE production_db\'], \n                   env={**os.environ, \'PGHOST\': \'localhost\'})\n    \n    # Create users table\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', \'\'\'\n        CREATE TABLE users (\n            user_id INTEGER PRIMARY KEY,\n            username VARCHAR(255) NOT NULL,\n            email VARCHAR(255) NOT NULL,\n            created_at TIMESTAMP,\n            status VARCHAR(50)\n        )\n    \'\'\'], env={**os.environ, \'PGHOST\': \'localhost\'})\n    \n    # Create transactions table\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', \'\'\'\n        CREATE TABLE transactions (\n            transaction_id INTEGER PRIMARY KEY,\n            user_id INTEGER,\n            amount DECIMAL(10, 2),\n            timestamp TIMESTAMP,\n            description TEXT\n        )\n    \'\'\'], env={**os.environ, \'PGHOST\': \'localhost\'})\n\n# Clean users data\ndef clean_users():\n    valid_statuses = [\'active\', \'inactive\', \'suspended\']\n    email_pattern = re.compile(r\'^[^@]+@[^@]+$\')\n    \n    clean_data = []\n    with open(\'/backup/users_export.csv\', \'r\') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Check for missing user_id\n            if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n                continue\n            \n            # Check for empty username\n            if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n                continue\n            \n            # Check for invalid email\n            email = row.get(\'email\', \'\').strip()\n            if not email or not email_pattern.match(email):\n                continue\n            \n            # Check for invalid status\n            status = row.get(\'status\', \'\').strip().lower()\n            if status not in valid_statuses:\n                continue\n            \n            clean_data.append({\n                \'user_id\': row[\'user_id\'].strip(),\n                \'username\': row[\'username\'].strip(),\n                \'email\': email,\n                \'created_at\': row.get(\'created_at\', \'\').strip(),\n                \'status\': status\n            })\n    \n    # Write cleaned data\n    with open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(clean_data)\n    \n    return clean_data\n\n# Clean transactions data\ndef clean_transactions():\n    with open(\'/backup/transactions_backup.json\', \'r\') as f:\n        transactions = json.load(f)\n    \n    valid_transactions = []\n    for txn in transactions:\n        # Check for null transaction_id\n        if txn.get(\'transaction_id\') is None:\n            continue\n        \n        # Check for null or negative amounts\n        amount = txn.get(\'amount\')\n        if amount is None or amount <= 0:\n            continue\n        \n        # Check for missing timestamp\n        if not txn.get(\'timestamp\'):\n            continue\n        \n        # Check for valid user_id (1-30)\n        user_id = txn.get(\'user_id\')\n        if user_id is None or user_id < 1 or user_id > 30:\n            continue\n        \n        valid_transactions.append(txn)\n    \n    # Write valid transactions\n    with open(\'/backup/valid_transactions.json\', \'w\') as f:\n        json.dump(valid_transactions, f, indent=2)\n        f.write(\'\\n\')\n    \n    return valid_transactions\n\n# Import data into database\ndef import_data(users, transactions):\n    # Import users\n    for user in users:\n        cmd = f"INSERT INTO users (user_id, username, email, created_at, status) VALUES ({user[\'user_id\']}, \'{user[\'username\']}\', \'{user[\'email\']}\', \'{user[\'created_at\']}\', \'{user[\'status\']}\')"\n        subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', cmd],\n                      env={**os.environ, \'PGHOST\': \'localhost\'})\n    \n    # Import transactions\n    for txn in transactions:\n        desc = txn.get(\'description\', \'\').replace("\'", "\'\'")\n        cmd = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\')"\n        subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', cmd],\n                      env={**os.environ, \'PGHOST\': \'localhost\'})\n\n# Generate database statistics\ndef generate_stats():\n    env = {**os.environ, \'PGHOST\': \'localhost\'}\n    \n    # Total users\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COUNT(*) FROM users\'],\n                           capture_output=True, text=True, env=env)\n    total_users = int(result.stdout.strip())\n    \n    # Active users\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', "SELECT COUNT(*) FROM users WHERE status = \'active\'"],\n                           capture_output=True, text=True, env=env)\n    active_users = int(result.stdout.strip())\n    \n    # Total transactions\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COUNT(*) FROM transactions\'],\n                           capture_output=True, text=True, env=env)\n    total_transactions = int(result.stdout.strip())\n    \n    # Total amount\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COALESCE(SUM(amount), 0) FROM transactions\'],\n                           capture_output=True, text=True, env=env)\n    total_amount = float(result.stdout.strip())\n    \n    # Average per user\n    if total_users > 0:\n        avg_per_user = total_amount / total_users\n    else:\n        avg_per_user = 0\n    \n    # Write stats\n    with open(\'/backup/db_stats.txt\', \'w\') as f:\n        f.write(\'DATABASE STATISTICS\\n\')\n        f.write(\'==================\\n\')\n        f.write(f\'Total Users: {total_users}\\n\')\n        f.write(f\'Active Users: {active_users}\\n\')\n        f.write(f\'Total Transactions: {total_transactions}\\n\')\n        f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n        f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\n# Generate top users report\ndef generate_top_users():\n    env = {**os.environ, \'PGHOST\': \'localhost\'}\n    \n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'\'\'\n        SELECT u.username, COALESCE(SUM(t.amount), 0) as total_amount\n        FROM users u\n        LEFT JOIN transactions t ON u.user_id = t.user_id\n        GROUP BY u.username\n        ORDER BY total_amount DESC, u.username ASC\n        LIMIT 10\n    \'\'\'], capture_output=True, text=True, env=env)\n    \n    lines = result.stdout.strip().split(\'\\n\')\n    \n    # Write top users\n    with open(\'/backup/top_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.writer(f)\n        writer.writerow([\'username\', \'total_amount\'])\n        for line in lines:\n            if line.strip():\n                parts = [p.strip() for p in line.split(\'|\')]\n                if len(parts) == 2:\n                    username = parts[0]\n                    amount = float(parts[1])\n                    writer.writerow([username, f\'{amount:.2f}\'])\n\n# Generate backup verification\ndef generate_verification():\n    with open(\'/backup/backup_metadata.log\', \'r\') as f:\n        content = f.read()\n    \n    backup_date = \'\'\n    tables_backed_up = \'\'\n    total_rows = \'\'\n    integrity_check = \'\'\n    \n    for line in content.split(\'\\n\'):\n        if \'Backup timestamp:\' in line:\n            backup_date = line.split(\'Backup timestamp:\')[1].strip()\n        elif \'Total tables backed up:\' in line:\n            tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n        elif \'Total rows backed up:\' in line:\n            total_rows = line.split(\'Total rows backed up:\')[1].strip()\n        elif \'Integrity check:\' in line:\n            integrity_check = line.split(\'Integrity check:\')[1].strip()\n    \n    with open(\'/backup/backup_verification.txt\', \'w\') as f:\n        f.write(\'BACKUP VERIFICATION\\n\')\n        f.write(\'===================\\n\')\n        f.write(f\'Backup Date: {backup_date}\\n\')\n        f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n        f.write(f\'Total Rows: {total_rows}\\n\')\n        f.write(f\'Integrity Check: {integrity_check}\\n\')\n\n# Main execution\nif __name__ == \'__main__\':\n    print(\'Creating database...\')\n    create_database()\n    \n    print(\'Cleaning users data...\')\n    users = clean_users()\n    \n    print(\'Cleaning transactions data...\')\n    transactions = clean_transactions()\n    \n    print(\'Importing data...\')\n    import_data(users, transactions)\n    \n    print(\'Generating statistics...\')\n    generate_stats()\n    \n    print(\'Generating top users report...\')\n    generate_top_users()\n    \n    print(\'Generating backup verification...\')\n    generate_verification()\n    \n    print(\'Creating recovery completion marker...\')\n    with open(\'/backup/recovery_complete.txt\', \'w\') as f:\n        f.write(\'RECOVERY_SUCCESSFUL\\n\')\n    \n    print(\'Recovery complete!\')\nEOF\n'
'python3 /backup/restore_database.py\n'
'C-c'
"sed -i 's/peer/trust/g' /etc/postgresql/*/main/pg_hba.conf\n"
"sed -i 's/md5/trust/g' /etc/postgresql/*/main/pg_hba.conf\n"
'service postgresql restart\n'
'python3 /backup/restore_database.py\n'
'C-c'
"cat /etc/postgresql/*/main/pg_hba.conf | grep -v '^#' | grep -v '^$'\n"
"sed -i 's/scram-sha-256/trust/g' /etc/postgresql/*/main/pg_hba.conf\n"
'service postgresql restart\n'
'python3 /backup/restore_database.py\n'
'cat > /backup/restore_database.py << \'EOF\'\nimport subprocess\nimport csv\nimport json\nimport re\nimport os\n\n# Create database and restore schema\ndef create_database():\n    # Drop and create database\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-c\', \'DROP DATABASE IF EXISTS production_db\'], \n                   env={**os.environ, \'PGHOST\': \'localhost\'})\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-c\', \'CREATE DATABASE production_db\'], \n                   env={**os.environ, \'PGHOST\': \'localhost\'})\n    \n    # Create users table\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', \'\'\'\n        CREATE TABLE users (\n            user_id INTEGER PRIMARY KEY,\n            username VARCHAR(255) NOT NULL,\n            email VARCHAR(255) NOT NULL,\n            created_at TIMESTAMP,\n            status VARCHAR(50)\n        )\n    \'\'\'], env={**os.environ, \'PGHOST\': \'localhost\'})\n    \n    # Create transactions table\n    subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', \'\'\'\n        CREATE TABLE transactions (\n            transaction_id INTEGER PRIMARY KEY,\n            user_id INTEGER,\n            amount DECIMAL(10, 2),\n            timestamp TIMESTAMP,\n            description TEXT\n        )\n    \'\'\'], env={**os.environ, \'PGHOST\': \'localhost\'})\n\n# Clean users data\ndef clean_users():\n    valid_statuses = [\'active\', \'inactive\', \'suspended\']\n    email_pattern = re.compile(r\'^[^@]+@[^@]+$\')\n    \n    clean_data = []\n    with open(\'/backup/users_export.csv\', \'r\') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Check for missing user_id\n            if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n                continue\n            \n            # Check for empty username\n            if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n                continue\n            \n            # Check for invalid email\n            email = row.get(\'email\', \'\').strip()\n            if not email or not email_pattern.match(email):\n                continue\n            \n            # Check for invalid status\n            status = row.get(\'status\', \'\').strip().lower()\n            if status not in valid_statuses:\n                continue\n            \n            clean_data.append({\n                \'user_id\': row[\'user_id\'].strip(),\n                \'username\': row[\'username\'].strip(),\n                \'email\': email,\n                \'created_at\': row.get(\'created_at\', \'\').strip(),\n                \'status\': status\n            })\n    \n    # Write cleaned data\n    with open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(clean_data)\n    \n    return clean_data\n\n# Clean transactions data\ndef clean_transactions():\n    with open(\'/backup/transactions_backup.json\', \'r\') as f:\n        transactions = json.load(f)\n    \n    valid_transactions = []\n    for txn in transactions:\n        # Check for null transaction_id\n        if txn.get(\'transaction_id\') is None:\n            continue\n        \n        # Check for null or negative amounts\n        amount = txn.get(\'amount\')\n        if amount is None:\n            continue\n        \n        # Convert to float if it\'s a string\n        try:\n            amount_val = float(amount)\n        except (ValueError, TypeError):\n            continue\n        \n        if amount_val <= 0:\n            continue\n        \n        # Check for missing timestamp\n        if not txn.get(\'timestamp\'):\n            continue\n        \n        # Check for valid user_id (1-30)\n        user_id = txn.get(\'user_id\')\n        if user_id is None:\n            continue\n        \n        try:\n            user_id_val = int(user_id)\n        except (ValueError, TypeError):\n            continue\n        \n        if user_id_val < 1 or user_id_val > 30:\n            continue\n        \n        valid_transactions.append(txn)\n    \n    # Write valid transactions\n    with open(\'/backup/valid_transactions.json\', \'w\') as f:\n        json.dump(valid_transactions, f, indent=2)\n        f.write(\'\\n\')\n    \n    return valid_transactions\n\n# Import data into database\ndef import_data(users, transactions):\n    # Import users\n    for user in users:\n        cmd = f"INSERT INTO users (user_id, username, email, created_at, status) VALUES ({user[\'user_id\']}, \'{user[\'username\']}\', \'{user[\'email\']}\', \'{user[\'created_at\']}\', \'{user[\'status\']}\')"\n        subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', cmd],\n                      env={**os.environ, \'PGHOST\': \'localhost\'}, capture_output=True)\n    \n    # Import transactions\n    for txn in transactions:\n        desc = txn.get(\'description\', \'\').replace("\'", "\'\'")\n        cmd = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\')"\n        subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-c\', cmd],\n                      env={**os.environ, \'PGHOST\': \'localhost\'}, capture_output=True)\n\n# Generate database statistics\ndef generate_stats():\n    env = {**os.environ, \'PGHOST\': \'localhost\'}\n    \n    # Total users\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COUNT(*) FROM users\'],\n                           capture_output=True, text=True, env=env)\n    total_users = int(result.stdout.strip())\n    \n    # Active users\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', "SELECT COUNT(*) FROM users WHERE status = \'active\'"],\n                           capture_output=True, text=True, env=env)\n    active_users = int(result.stdout.strip())\n    \n    # Total transactions\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COUNT(*) FROM transactions\'],\n                           capture_output=True, text=True, env=env)\n    total_transactions = int(result.stdout.strip())\n    \n    # Total amount\n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'SELECT COALESCE(SUM(amount), 0) FROM transactions\'],\n                           capture_output=True, text=True, env=env)\n    total_amount = float(result.stdout.strip())\n    \n    # Average per user\n    if total_users > 0:\n        avg_per_user = total_amount / total_users\n    else:\n        avg_per_user = 0\n    \n    # Write stats\n    with open(\'/backup/db_stats.txt\', \'w\') as f:\n        f.write(\'DATABASE STATISTICS\\n\')\n        f.write(\'==================\\n\')\n        f.write(f\'Total Users: {total_users}\\n\')\n        f.write(f\'Active Users: {active_users}\\n\')\n        f.write(f\'Total Transactions: {total_transactions}\\n\')\n        f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n        f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\n# Generate top users report\ndef generate_top_users():\n    env = {**os.environ, \'PGHOST\': \'localhost\'}\n    \n    result = subprocess.run([\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-t\', \'-c\', \'\'\'\n        SELECT u.username, COALESCE(SUM(t.amount), 0) as total_amount\n        FROM users u\n        LEFT JOIN transactions t ON u.user_id = t.user_id\n        GROUP BY u.username\n        ORDER BY total_amount DESC, u.username ASC\n        LIMIT 10\n    \'\'\'], capture_output=True, text=True, env=env)\n    \n    lines = result.stdout.strip().split(\'\\n\')\n    \n    # Write top users\n    with open(\'/backup/top_users.csv\', \'w\', newline=\'\') as f:\n        writer = csv.writer(f)\n        writer.writerow([\'username\', \'total_amount\'])\n        for line in lines:\n            if line.strip():\n                parts = [p.strip() for p in line.split(\'|\')]\n                if len(parts) == 2:\n                    username = parts[0]\n                    amount = float(parts[1])\n                    writer.writerow([username, f\'{amount:.2f}\'])\n\n# Generate backup verification\ndef generate_verification():\n    with open(\'/backup/backup_metadata.log\', \'r\') as f:\n        content = f.read()\n    \n    backup_date = \'\'\n    tables_backed_up = \'\'\n    total_rows = \'\'\n    integrity_check = \'\'\n    \n    for line in content.split(\'\\n\'):\n        if \'Backup timestamp:\' in line:\n            backup_date = line.split(\'Backup timestamp:\')[1].strip()\n        elif \'Total tables backed up:\' in line:\n            tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n        elif \'Total rows backed up:\' in line:\n            total_rows = line.split(\'Total rows backed up:\')[1].strip()\n        elif \'Integrity check:\' in line:\n            integrity_check = line.split(\'Integrity check:\')[1].strip()\n    \n    with open(\'/backup/backup_verification.txt\', \'w\') as f:\n        f.write(\'BACKUP VERIFICATION\\n\')\n        f.write(\'===================\\n\')\n        f.write(f\'Backup Date: {backup_date}\\n\')\n        f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n        f.write(f\'Total Rows: {total_rows}\\n\')\n        f.write(f\'Integrity Check: {integrity_check}\\n\')\n\n# Main execution\nif __name__ == \'__main__\':\n    print(\'Creating database...\')\n    create_database()\n    \n    print(\'Cleaning users data...\')\n    users = clean_users()\n    \n    print(\'Cleaning transactions data...\')\n    transactions = clean_transactions()\n    \n    print(\'Importing data...\')\n    import_data(users, transactions)\n    \n    print(\'Generating statistics...\')\n    generate_stats()\n    \n    print(\'Generating top users report...\')\n    generate_top_users()\n    \n    print(\'Generating backup verification...\')\n    generate_verification()\n    \n    print(\'Creating recovery completion marker...\')\n    with open(\'/backup/recovery_complete.txt\', \'w\') as f:\n        f.write(\'RECOVERY_SUCCESSFUL\\n\')\n    \n    print(\'Recovery complete!\')\nEOF\n'
'python3 /backup/restore_database.py\n'
'ls -la /backup/*.csv /backup/*.json /backup/*.txt 2>/dev/null\n'
"echo '=== clean_users.csv ==='\n"
'head -5 /backup/clean_users.csv\n'
"echo '=== valid_transactions.json ==='\n"
'head -20 /backup/valid_transactions.json\n'
"echo '=== db_stats.txt ==='\n"
'cat /backup/db_stats.txt\n'
"echo '=== top_users.csv ==='\n"
'cat /backup/top_users.csv\n'
"echo '=== backup_verification.txt ==='\n"
'cat /backup/backup_verification.txt\n'
"echo '=== recovery_complete.txt ==='\n"
'cat /backup/recovery_complete.txt\n'
"psql -U postgres -d production_db -c '\\dt'\n"
"psql -U postgres -d production_db -c 'SELECT COUNT(*) FROM users;'\n"
"psql -U postgres -d production_db -c 'SELECT COUNT(*) FROM transactions;'\n"
'wc -l /backup/clean_users.csv\n'
'head -3 /backup/clean_users.csv\n'
'tail -1 /backup/clean_users.csv | od -c | head -2\n'
'wc -l /backup/valid_transactions.json\n'
'tail -3 /backup/valid_transactions.json\n'
'for file in /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt; do echo "=== $file ==="  && tail -c 1 "$file" | od -An -tx1; done\n'
"echo 'All files verified!'\n"
'wc -l /backup/top_users.csv\n'
'cat /backup/top_users.csv\n'
"echo '=== Verifying database can be queried ==='\n"
'psql -U postgres -d production_db -c "SELECT username, email, status FROM users LIMIT 3;"\n'
'psql -U postgres -d production_db -c "SELECT transaction_id, user_id, amount FROM transactions LIMIT 3;"\n'
"echo '=== All tasks completed successfully! ==='\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
