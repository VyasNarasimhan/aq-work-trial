['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
'wc -l /backup/users_export.csv\n'
'cat /backup/users_export.csv\n'
'python3 -c "import json; data = json.load(open(\'/backup/transactions_backup.json\')); print(f\'Total transactions: {len(data)}\'); print(\'Sample transactions:\'); import pprint; pprint.pprint(data[:5]); print(\'...\'); pprint.pprint(data[-5:])"\n'
'cat > /tmp/clean_and_import.py << \'EOF\'\nimport csv\nimport json\nimport re\nimport psycopg2\n\n# Clean users CSV\nvalid_users = []\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for valid user_id\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        # Check for valid username\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        # Check for valid email (must contain @)\n        if not row.get(\'email\') or \'@\' not in row[\'email\']:\n            continue\n        # Check for valid status\n        if row.get(\'status\') not in [\'active\', \'inactive\', \'suspended\']:\n            continue\n        valid_users.append(row)\n\n# Write cleaned users\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n    writer.writeheader()\n    writer.writerows(valid_users)\n\nprint(f\'Cleaned users: {len(valid_users)} valid rows\')\n\n# Clean transactions JSON\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get(\'transaction_id\') is None:\n        continue\n    # Check for null or negative amounts\n    if txn.get(\'amount\') is None or txn[\'amount\'] <= 0:\n        continue\n    # Check for missing timestamp\n    if not txn.get(\'timestamp\'):\n        continue\n    # Check for valid user_id (1-30)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint(f\'Valid transactions: {len(valid_transactions)} valid records\')\n\n# Connect to database and import data\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'localhost\'\n)\ncur = conn.cursor()\n\n# Import users\nfor user in valid_users:\n    cur.execute(\n        "INSERT INTO users (user_id, username, email, created_at, status) VALUES (%s, %s, %s, %s, %s)",\n        (user[\'user_id\'], user[\'username\'], user[\'email\'], user[\'created_at\'], user[\'status\'])\n    )\n\nprint(f\'Imported {len(valid_users)} users into database\')\n\n# Import transactions\nfor txn in valid_transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn.get(\'description\', \'\'))\n    )\n\nprint(f\'Imported {len(valid_transactions)} transactions into database\')\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint(\'Data cleaning and import completed successfully\')\nEOF\n'
'python3 /tmp/clean_and_import.py\n'
'ls -la /backup/*.csv /backup/*.json\n'
'head -5 /backup/clean_users.csv\n'
'wc -l /backup/clean_users.csv\n'
"psql -U postgres -d production_db -c 'SELECT COUNT(*) FROM users;'\n"
"psql -U postgres -d production_db -c 'SELECT COUNT(*) FROM transactions;'\n"
"cat > /tmp/clean_data.py << 'EOF'\nimport csv\nimport json\n\n# Clean users CSV\nvalid_users = []\nwith open('/backup/users_export.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for valid user_id\n        if not row.get('user_id') or not row['user_id'].strip():\n            continue\n        # Check for valid username\n        if not row.get('username') or not row['username'].strip():\n            continue\n        # Check for valid email (must contain @)\n        if not row.get('email') or '@' not in row['email']:\n            continue\n        # Check for valid status\n        if row.get('status') not in ['active', 'inactive', 'suspended']:\n            continue\n        valid_users.append(row)\n\n# Write cleaned users\nwith open('/backup/clean_users.csv', 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=['user_id', 'username', 'email', 'created_at', 'status'])\n    writer.writeheader()\n    writer.writerows(valid_users)\n\nprint(f'Cleaned users: {len(valid_users)} valid rows')\n\n# Clean transactions JSON\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    # Check for null or negative amounts\n    if txn.get('amount') is None or txn['amount'] <= 0:\n        continue\n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    # Check for valid user_id (1-30)\n    if txn.get('user_id') is None or txn['user_id'] < 1 or txn['user_id'] > 30:\n        continue\n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Valid transactions: {len(valid_transactions)} valid records')\nEOF\n"
'python3 /tmp/clean_data.py\n'
'wc -l /backup/clean_users.csv /backup/valid_transactions.json\n'
"cat > /tmp/clean_transactions.py << 'EOF'\nimport json\n\n# Clean transactions JSON\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    \n    # Check for null or negative amounts\n    amount = txn.get('amount')\n    if amount is None:\n        continue\n    \n    # Convert amount to float if it's a string\n    try:\n        amount_float = float(amount)\n        if amount_float <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    \n    # Check for valid user_id (1-30)\n    user_id = txn.get('user_id')\n    if user_id is None:\n        continue\n    \n    try:\n        user_id_int = int(user_id)\n        if user_id_int < 1 or user_id_int > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Valid transactions: {len(valid_transactions)} valid records')\nEOF\n"
'python3 /tmp/clean_transactions.py\n'
'ls -la /backup/clean_users.csv /backup/valid_transactions.json\n'
'su - postgres -c "psql -d production_db -c \'\\\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'\\\\d users\'"\n'
'su - postgres -c "psql -d production_db -c \'\\\\d transactions\'"\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
"cat > /tmp/import_transactions.py << 'EOF'\nimport json\nimport csv\n\n# Read transactions from JSON\nwith open('/backup/valid_transactions.json', 'r') as f:\n    transactions = json.load(f)\n\n# Write to CSV for import\nwith open('/tmp/transactions.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['transaction_id', 'user_id', 'amount', 'timestamp', 'description'])\n    for txn in transactions:\n        writer.writerow([\n            txn['transaction_id'],\n            txn['user_id'],\n            txn['amount'],\n            txn['timestamp'],\n            txn.get('description', '')\n        ])\n\nprint(f'Wrote {len(transactions)} transactions to CSV')\nEOF\n"
'python3 /tmp/import_transactions.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY transactions(transaction_id, user_id, amount, timestamp, description) FROM \'/tmp/transactions.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /tmp/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\n# Generate database statistics report\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', \n     "psql -d production_db -t -A -F\'|\' -c \'SELECT COUNT(*) FROM users; SELECT COUNT(*) FROM users WHERE status = \'\\\'\'active\'\\\'\'; SELECT COUNT(*) FROM transactions; SELECT SUM(amount) FROM transactions; SELECT SUM(amount)::numeric / COUNT(DISTINCT user_id) FROM transactions;\'"],\n    capture_output=True, text=True\n)\n\nlines = result.stdout.strip().split(\'\\n\')\ntotal_users = lines[0]\nactive_users = lines[1]\ntotal_transactions = lines[2]\ntotal_amount = float(lines[3])\navg_per_user = float(lines[4])\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n    f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\nprint(\'Generated db_stats.txt\')\n\n# Generate top users report\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\',\n     "psql -d production_db -t -A -F\',\' -c \\"SELECT u.username, COALESCE(SUM(t.amount), 0)::numeric(10,2) as total_amount FROM users u LEFT JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10;\\""],\n    capture_output=True, text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(\'username,total_amount\\n\')\n    f.write(result.stdout)\n\nprint(\'Generated top_users.csv\')\n\n# Generate backup verification report\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\nbackup_date = \'\'\ntables_backed_up = \'\'\ntotal_rows = \'\'\nintegrity_check = \'\'\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write(\'BACKUP VERIFICATION\\n\')\n    f.write(\'===================\\n\')\n    f.write(f\'Backup Date: {backup_date}\\n\')\n    f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n    f.write(f\'Total Rows: {total_rows}\\n\')\n    f.write(f\'Integrity Check: {integrity_check}\\n\')\n\nprint(\'Generated backup_verification.txt\')\n\n# Create recovery complete marker\nwith open(\'/backup/recovery_complete.txt\', \'w\') as f:\n    f.write(\'RECOVERY_SUCCESSFUL\\n\')\n\nprint(\'Created recovery_complete.txt\')\nprint(\'All reports generated successfully!\')\nEOF\n'
'python3 /tmp/generate_reports.py\n'
'cat > /tmp/generate_reports.py << \'EOF\'\nimport subprocess\n\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f"psql -d production_db -t -A -c \\"{query}\\""],\n        capture_output=True, text=True\n    )\n    return result.stdout.strip()\n\n# Generate database statistics report\ntotal_users = run_query(\'SELECT COUNT(*) FROM users;\')\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query(\'SELECT COUNT(*) FROM transactions;\')\ntotal_amount = float(run_query(\'SELECT COALESCE(SUM(amount), 0) FROM transactions;\'))\navg_per_user = float(run_query(\'SELECT COALESCE(SUM(amount)::numeric / COUNT(DISTINCT user_id), 0) FROM transactions;\'))\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n    f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\nprint(\'Generated db_stats.txt\')\n\n# Generate top users report\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\',\n     "psql -d production_db -t -A -F\',\' -c \\"SELECT u.username, COALESCE(SUM(t.amount), 0)::numeric(10,2) as total_amount FROM users u LEFT JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10;\\""],\n    capture_output=True, text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(\'username,total_amount\\n\')\n    f.write(result.stdout)\n\nprint(\'Generated top_users.csv\')\n\n# Generate backup verification report\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\nbackup_date = \'\'\ntables_backed_up = \'\'\ntotal_rows = \'\'\nintegrity_check = \'\'\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write(\'BACKUP VERIFICATION\\n\')\n    f.write(\'===================\\n\')\n    f.write(f\'Backup Date: {backup_date}\\n\')\n    f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n    f.write(f\'Total Rows: {total_rows}\\n\')\n    f.write(f\'Integrity Check: {integrity_check}\\n\')\n\nprint(\'Generated backup_verification.txt\')\n\n# Create recovery complete marker\nwith open(\'/backup/recovery_complete.txt\', \'w\') as f:\n    f.write(\'RECOVERY_SUCCESSFUL\\n\')\n\nprint(\'Created recovery_complete.txt\')\nprint(\'All reports generated successfully!\')\nEOF\n'
'python3 /tmp/generate_reports.py\n'
'ls -la /backup/*.csv /backup/*.json /backup/*.txt\n'
"echo '=== clean_users.csv ==='\n"
"head -5 /backup/clean_users.csv && echo '...' && wc -l /backup/clean_users.csv\n"
"echo '=== valid_transactions.json ==='\n"
'head -20 /backup/valid_transactions.json\n'
"echo '=== db_stats.txt ==='\n"
'cat /backup/db_stats.txt\n'
"echo '=== top_users.csv ==='\n"
'cat /backup/top_users.csv\n'
"echo '=== backup_verification.txt ==='\n"
'cat /backup/backup_verification.txt\n'
"echo '=== recovery_complete.txt ==='\n"
'cat /backup/recovery_complete.txt\n'
"echo '=== Verification: Check all files exist ==='\n"
'ls -lh /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt\n'
"echo '=== Verification: Check database ==='\n"
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as transaction_count FROM transactions;\'"\n'
'wc -l /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt\n'
'tail -c 20 /backup/valid_transactions.json | od -c\n'
'tail -c 20 /backup/clean_users.csv | od -c\n'
'tail -c 20 /backup/db_stats.txt | od -c\n'
'tail -c 20 /backup/top_users.csv | od -c\n'
'tail -c 20 /backup/backup_verification.txt | od -c\n'
'tail -c 20 /backup/recovery_complete.txt | od -c\n'
"dos2unix /backup/clean_users.csv 2>/dev/null || sed -i 's/\\r$//' /backup/clean_users.csv\n"
'tail -c 20 /backup/clean_users.csv | od -c\n'
"echo '=== Final Verification Summary ==='\n"
'echo \'Database: production_db\' && su - postgres -c "psql -l | grep production_db"\n'
'echo \'Tables:\' && su - postgres -c "psql -d production_db -c \'\\\\dt\'"\n'
'echo \'Row counts:\' && su - postgres -c "psql -d production_db -c \'SELECT (SELECT COUNT(*) FROM users) as users, (SELECT COUNT(*) FROM transactions) as transactions;\'"\n'
"echo 'All required files:' && ls -1 /backup/*.csv /backup/*.json /backup/*.txt | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification|recovery_complete)'\n"
"echo '=== Sample of clean_users.csv (first 3 data rows) ==='\n"
'head -4 /backup/clean_users.csv\n'
"echo '=== Verify top_users amounts have 2 decimal places ==='\n"
'tail -3 /backup/top_users.csv\n'
"echo '=== Verify JSON is valid ==='\n"
'python3 -c "import json; data=json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid JSON with {len(data)} transactions\'); print(\'First transaction:\', data[0])"\n'
"echo '=== All tasks completed successfully! ==='\n"
"echo '=== Final Format Verification ==='\n"
"echo 'File sizes and line counts:' && wc -l /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt\n"
'echo \'Checking all files end with newline:\' && for f in /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt; do echo -n "$f: "; tail -c 1 "$f" | od -An -tx1 | grep -q \'0a\' && echo \'OK (ends with newline)\' || echo \'MISSING NEWLINE\'; done\n'
'echo \'Database accessible and contains data:\' && su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as total_users FROM users; SELECT COUNT(*) as total_transactions FROM transactions;\'"\n'
"echo '=== RECOVERY COMPLETE - All requirements met ==='\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
