['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
'service postgresql start\n'
'cat /backup/database.sql\n'
'psql -U postgres -c "CREATE DATABASE production_db;"\n'
'psql -U postgres -d production_db -f /backup/database.sql\n'
'psql -U postgres -d production_db -c "\\dt"\n'
'sudo -u postgres psql -c "CREATE DATABASE production_db;"\n'
'sudo -u postgres psql -d production_db -f /backup/database.sql\n'
'sudo -u postgres psql -d production_db -c "\\dt"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'cat > /tmp/clean_users.py << \'EOF\'\nimport csv\nimport re\n\n# Read and clean users data\nvalid_statuses = {\'active\', \'inactive\', \'suspended\'}\nemail_pattern = re.compile(r\'.+@.+\')\n\nwith open(\'/backup/users_export.csv\', \'r\') as infile:\n    reader = csv.DictReader(infile)\n    \n    clean_rows = []\n    for row in reader:\n        # Check if user_id exists and is not empty\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        \n        # Check if username exists and is not empty\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        \n        # Check if email is valid (contains @)\n        if not row.get(\'email\') or not email_pattern.match(row[\'email\']):\n            continue\n        \n        # Check if status is valid\n        if not row.get(\'status\') or row[\'status\'].strip() not in valid_statuses:\n            continue\n        \n        clean_rows.append(row)\n\n# Write cleaned data\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as outfile:\n    if clean_rows:\n        fieldnames = [\'user_id\', \'username\', \'email\', \'created_at\', \'status\']\n        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(clean_rows)\n\nprint(f"Cleaned {len(clean_rows)} valid user records")\nEOF\n'
'python3 /tmp/clean_users.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH CSV HEADER;\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /tmp/clean_transactions.py << \'EOF\'\nimport json\n\n# Read transactions data\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check transaction_id is not null\n    if txn.get(\'transaction_id\') is None:\n        continue\n    \n    # Check amount is not null and is positive\n    if txn.get(\'amount\') is None or txn[\'amount\'] <= 0:\n        continue\n    \n    # Check timestamp exists\n    if not txn.get(\'timestamp\'):\n        continue\n    \n    # Check user_id is valid (1-30)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    \n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint(f"Found {len(valid_transactions)} valid transactions")\nEOF\n'
'python3 /tmp/clean_transactions.py\n'
'cat > /tmp/clean_transactions.py << \'EOF\'\nimport json\n\n# Read transactions data\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check transaction_id is not null\n    if txn.get(\'transaction_id\') is None:\n        continue\n    \n    # Check amount is not null and is positive\n    amount = txn.get(\'amount\')\n    if amount is None:\n        continue\n    \n    try:\n        amount_float = float(amount)\n        if amount_float <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check timestamp exists\n    if not txn.get(\'timestamp\'):\n        continue\n    \n    # Check user_id is valid (1-30)\n    user_id = txn.get(\'user_id\')\n    if user_id is None:\n        continue\n    \n    try:\n        user_id_int = int(user_id)\n        if user_id_int < 1 or user_id_int > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(txn)\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint(f"Found {len(valid_transactions)} valid transactions")\nEOF\n'
'python3 /tmp/clean_transactions.py\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /tmp/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Insert transactions\nfor txn in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn[\'description\'])\n    )\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint(f"Imported {len(transactions)} transactions")\nEOF\n'
'python3 /tmp/import_transactions.py\n'
'cat > /tmp/import_transactions.py << \'EOF\'\nimport json\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Generate SQL file\nwith open(\'/tmp/transactions.sql\', \'w\') as f:\n    for txn in transactions:\n        # Escape single quotes in description\n        desc = txn[\'description\'].replace("\'", "\'\'")\n        sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\');\\n"\n        f.write(sql)\n\nprint(f"Generated SQL for {len(transactions)} transactions")\nEOF\n'
'python3 /tmp/import_transactions.py\n'
'su - postgres -c "psql -d production_db -f /tmp/transactions.sql" > /dev/null 2>&1\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /tmp/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\n# Generate db_stats.txt\nprint("Generating database statistics...")\n\n# Get total users\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT COUNT(*) FROM users;\'"],\n    capture_output=True, text=True\n)\ntotal_users = result.stdout.strip()\n\n# Get active users\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \\"SELECT COUNT(*) FROM users WHERE status = \'active\';\\""],\n    capture_output=True, text=True\n)\nactive_users = result.stdout.strip()\n\n# Get total transactions\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT COUNT(*) FROM transactions;\'"],\n    capture_output=True, text=True\n)\ntotal_transactions = result.stdout.strip()\n\n# Get total amount\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;\'"],\n    capture_output=True, text=True\n)\ntotal_amount = result.stdout.strip()\n\n# Get average per user\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT ROUND(AVG(user_total)::numeric, 2) FROM (SELECT user_id, SUM(amount) as user_total FROM transactions GROUP BY user_id) as user_totals;\'"],\n    capture_output=True, text=True\n)\navg_per_user = result.stdout.strip()\n\n# Write db_stats.txt\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write("DATABASE STATISTICS\\n")\n    f.write("==================\\n")\n    f.write(f"Total Users: {total_users}\\n")\n    f.write(f"Active Users: {active_users}\\n")\n    f.write(f"Total Transactions: {total_transactions}\\n")\n    f.write(f"Total Amount: {total_amount}\\n")\n    f.write(f"Average per User: {avg_per_user}\\n")\n\nprint("Database statistics generated")\nEOF\n'
'python3 /tmp/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'su - postgres -c "psql -d production_db -t -A -F\',\' -c \'SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10;\'" > /backup/top_users.csv\n'
"cat > /tmp/fix_top_users.py << 'EOF'\nimport csv\n\n# Read the data\nwith open('/backup/top_users.csv', 'r') as f:\n    lines = f.readlines()\n\n# Write with proper header\nwith open('/backup/top_users.csv', 'w', newline='') as f:\n    f.write('username,total_amount\\n')\n    for line in lines:\n        f.write(line)\n\nEOF\n"
'python3 /tmp/fix_top_users.py\n'
'cat /backup/top_users.csv\n'
"cat > /tmp/generate_verification.py << 'EOF'\nimport re\n\n# Read the log file\nwith open('/backup/backup_metadata.log', 'r') as f:\n    log_content = f.read()\n\n# Extract information\nbackup_timestamp = ''\ntables_backed_up = ''\ntotal_rows = ''\nintegrity_check = ''\n\nfor line in log_content.split('\\n'):\n    if 'Backup timestamp:' in line:\n        backup_timestamp = line.split('Backup timestamp:')[1].strip()\n    elif 'Total tables backed up:' in line:\n        tables_backed_up = line.split('Total tables backed up:')[1].strip()\n    elif 'Total rows backed up:' in line:\n        total_rows = line.split('Total rows backed up:')[1].strip()\n    elif 'Integrity check:' in line:\n        integrity_check = line.split('Integrity check:')[1].strip()\n\n# Write backup verification\nwith open('/backup/backup_verification.txt', 'w') as f:\n    f.write('BACKUP VERIFICATION\\n')\n    f.write('===================\\n')\n    f.write(f'Backup Date: {backup_timestamp}\\n')\n    f.write(f'Tables Backed Up: {tables_backed_up}\\n')\n    f.write(f'Total Rows: {total_rows}\\n')\n    f.write(f'Integrity Check: {integrity_check}\\n')\n\nprint('Backup verification generated')\nEOF\n"
'python3 /tmp/generate_verification.py\n'
'cat /backup/backup_verification.txt\n'
"echo 'RECOVERY_SUCCESSFUL' > /backup/recovery_complete.txt\n"
'ls -la /backup/*.txt /backup/*.csv /backup/*.json\n'
"echo '--- Verifying all files ---'\n"
'wc -l /backup/clean_users.csv /backup/top_users.csv\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as users FROM users; SELECT COUNT(*) as transactions FROM transactions;\'"\n'
'tail -c 20 /backup/clean_users.csv | od -c\n'
'tail -c 20 /backup/valid_transactions.json | od -c\n'
'tail -c 20 /backup/db_stats.txt | od -c\n'
'tail -c 20 /backup/top_users.csv | od -c\n'
'tail -c 20 /backup/backup_verification.txt | od -c\n'
'tail -c 20 /backup/recovery_complete.txt | od -c\n'
"dos2unix /backup/clean_users.csv 2>/dev/null || sed -i 's/\\r$//' /backup/clean_users.csv\n"
'tail -c 20 /backup/clean_users.csv | od -c\n'
'head -3 /backup/clean_users.csv\n'
'head -5 /backup/valid_transactions.json\n'
"echo '=== Final Summary ==='\n"
"echo 'Database: production_db with users and transactions tables created'\n"
"echo 'Files created: clean_users.csv, valid_transactions.json, db_stats.txt, top_users.csv, backup_verification.txt, recovery_complete.txt'\n"
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
"ls -lh /backup/*.csv /backup/*.json /backup/*.txt | grep -v 'export\\|backup\\|metadata\\|database'\n"
"echo '=== Task Completion Verified ==='\n"
'cat /backup/recovery_complete.txt\n'
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
