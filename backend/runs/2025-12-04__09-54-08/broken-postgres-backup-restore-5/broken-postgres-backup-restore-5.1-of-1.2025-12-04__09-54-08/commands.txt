['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -10 /backup/users_export.csv\n'
'head -10 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
"psql -U postgres -d production_db -c '\\d users'\n"
"psql -U postgres -d production_db -c '\\d transactions'\n"
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d users\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d transactions\'"\n'
"cat > /backup/clean_data.py << 'EOF'\nimport csv\nimport json\nimport re\n\n# Clean users data\nvalid_statuses = {'active', 'inactive', 'suspended'}\nclean_users = []\n\nwith open('/backup/users_export.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for missing user_id\n        if not row.get('user_id') or not row['user_id'].strip():\n            continue\n        # Check for empty username\n        if not row.get('username') or not row['username'].strip():\n            continue\n        # Check for invalid email (must contain @)\n        if not row.get('email') or '@' not in row['email']:\n            continue\n        # Check for invalid status\n        if not row.get('status') or row['status'].strip() not in valid_statuses:\n            continue\n        clean_users.append(row)\n\n# Write clean users to CSV\nwith open('/backup/clean_users.csv', 'w', newline='') as f:\n    if clean_users:\n        fieldnames = ['user_id', 'username', 'email', 'created_at', 'status']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(clean_users)\n\nprint(f'Cleaned users: {len(clean_users)} valid rows')\n\n# Clean transactions data\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    # Check for null or negative amount\n    if txn.get('amount') is None or txn['amount'] <= 0:\n        continue\n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    # Check for invalid user_id (must be 1-30)\n    if txn.get('user_id') is None or txn['user_id'] < 1 or txn['user_id'] > 30:\n        continue\n    valid_transactions.append(txn)\n\n# Write valid transactions to JSON\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Valid transactions: {len(valid_transactions)} valid entries')\nEOF\n"
'python3 /backup/clean_data.py\n'
"cat > /backup/clean_data.py << 'EOF'\nimport csv\nimport json\nimport re\n\n# Clean users data\nvalid_statuses = {'active', 'inactive', 'suspended'}\nclean_users = []\n\nwith open('/backup/users_export.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for missing user_id\n        if not row.get('user_id') or not row['user_id'].strip():\n            continue\n        # Check for empty username\n        if not row.get('username') or not row['username'].strip():\n            continue\n        # Check for invalid email (must contain @)\n        if not row.get('email') or '@' not in row['email']:\n            continue\n        # Check for invalid status\n        if not row.get('status') or row['status'].strip() not in valid_statuses:\n            continue\n        clean_users.append(row)\n\n# Write clean users to CSV\nwith open('/backup/clean_users.csv', 'w', newline='') as f:\n    if clean_users:\n        fieldnames = ['user_id', 'username', 'email', 'created_at', 'status']\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(clean_users)\n\nprint(f'Cleaned users: {len(clean_users)} valid rows')\n\n# Clean transactions data\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    \n    # Check for null or negative amount\n    amount = txn.get('amount')\n    if amount is None:\n        continue\n    try:\n        amount_float = float(amount)\n        if amount_float <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    \n    # Check for invalid user_id (must be 1-30)\n    user_id = txn.get('user_id')\n    if user_id is None:\n        continue\n    try:\n        user_id_int = int(user_id)\n        if user_id_int < 1 or user_id_int > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(txn)\n\n# Write valid transactions to JSON\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Valid transactions: {len(valid_transactions)} valid entries')\nEOF\n"
'python3 /backup/clean_data.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Load valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Insert transactions\nfor txn in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn.get(\'description\', \'\'))\n    )\n\nconn.commit()\nprint(f\'Imported {len(transactions)} transactions\')\n\ncur.close()\nconn.close()\nEOF\n'
'python3 /backup/import_transactions.py\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\n\n# Load valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Generate SQL INSERT statements\nwith open(\'/backup/insert_transactions.sql\', \'w\') as f:\n    for txn in transactions:\n        description = txn.get(\'description\', \'\').replace("\'", "\'\'")\n        sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{description}\');\\n"\n        f.write(sql)\n\nprint(f\'Generated SQL for {len(transactions)} transactions\')\nEOF\n'
'python3 /backup/import_transactions.py\n'
'su - postgres -c "psql -d production_db -f /backup/insert_transactions.sql"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /backup/generate_reports.py << \'EOF\'\nimport subprocess\nimport re\n\n# Generate database statistics report\nprint(\'Generating database statistics...\')\n\n# Query total users\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT COUNT(*) FROM users;\'"],\n    capture_output=True, text=True\n)\ntotal_users = result.stdout.strip()\n\n# Query active users\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \\"SELECT COUNT(*) FROM users WHERE status = \'active\';\\""],\n    capture_output=True, text=True\n)\nactive_users = result.stdout.strip()\n\n# Query total transactions\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT COUNT(*) FROM transactions;\'"],\n    capture_output=True, text=True\n)\ntotal_transactions = result.stdout.strip()\n\n# Query total amount\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT SUM(amount) FROM transactions;\'"],\n    capture_output=True, text=True\n)\ntotal_amount = float(result.stdout.strip())\n\n# Query average per user\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', "psql -d production_db -t -c \'SELECT AVG(total) FROM (SELECT user_id, SUM(amount) as total FROM transactions GROUP BY user_id) subq;\'"],\n    capture_output=True, text=True\n)\navg_per_user = float(result.stdout.strip())\n\n# Write db_stats.txt\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount:.2f}\\n\')\n    f.write(f\'Average per User: {avg_per_user:.2f}\\n\')\n\nprint(\'Database statistics report created\')\n\n# Generate top users report\nprint(\'Generating top users report...\')\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', \n     "psql -d production_db -t -A -F\',\' -c \\"SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10;\\""],\n    capture_output=True, text=True\n)\n\ntop_users_data = result.stdout.strip()\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(\'username,total_amount\\n\')\n    f.write(top_users_data)\n    f.write(\'\\n\')\n\nprint(\'Top users report created\')\n\n# Generate backup verification report\nprint(\'Generating backup verification report...\')\n\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\n# Extract information\nbackup_date = None\ntables_backed_up = None\ntotal_rows = None\nintegrity_check = None\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write(\'BACKUP VERIFICATION\\n\')\n    f.write(\'===================\\n\')\n    f.write(f\'Backup Date: {backup_date}\\n\')\n    f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n    f.write(f\'Total Rows: {total_rows}\\n\')\n    f.write(f\'Integrity Check: {integrity_check}\\n\')\n\nprint(\'Backup verification report created\')\nEOF\n'
'python3 /backup/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'cat /backup/backup_verification.txt\n'
'head -5 /backup/clean_users.csv\n'
'head -20 /backup/valid_transactions.json\n'
'wc -l /backup/top_users.csv\n'
'cat /backup/top_users.csv\n'
'cat /backup/db_stats.txt\n'
"ls -la /backup/*.txt /backup/*.csv /backup/*.json | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification)'\n"
"echo 'RECOVERY_SUCCESSFUL' > /backup/recovery_complete.txt\n"
'cat /backup/recovery_complete.txt\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as txn_count FROM transactions;\'"\n'
"ls -1 /backup/ | grep -E '^(clean_users\\.csv|valid_transactions\\.json|db_stats\\.txt|top_users\\.csv|backup_verification\\.txt|recovery_complete\\.txt)$'\n"
"echo 'All tasks completed successfully!'\n"
"echo 'Database recovery process completed. All requirements satisfied.'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
