['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'head -20 database.sql\n'
'head -10 users_export.csv\n'
'head -10 transactions_backup.json\n'
'cat backup_metadata.log\n'
'service postgresql start\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
"psql -U postgres -d production_db -c '\\d users'\n"
"psql -U postgres -d production_db -c '\\d transactions'\n"
'wc -l users_export.csv\n'
'cat users_export.csv\n'
'cat transactions_backup.json | python3 -m json.tool | head -50\n'
'python3 -c "import json; data=json.load(open(\'transactions_backup.json\')); print(f\'Total transactions: {len(data)}\'); print(\'Sample records:\'); [print(data[i]) for i in range(min(5, len(data)))]"\n'
'cat > clean_and_import_users.py << \'EOF\'\nimport csv\nimport re\nimport psycopg2\n\n# Valid status values\nVALID_STATUSES = {\'active\', \'inactive\', \'suspended\'}\n\ndef is_valid_email(email):\n    """Check if email contains @"""\n    return email and \'@\' in email\n\ndef clean_users():\n    """Clean users_export.csv and return valid rows"""\n    valid_rows = []\n    \n    with open(\'/backup/users_export.csv\', \'r\') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Check for missing user_id\n            if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n                continue\n            \n            # Check for empty username\n            if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n                continue\n            \n            # Check for invalid email (must contain @)\n            if not is_valid_email(row.get(\'email\', \'\')):\n                continue\n            \n            # Check for invalid status\n            if row.get(\'status\', \'\').strip().lower() not in VALID_STATUSES:\n                continue\n            \n            valid_rows.append(row)\n    \n    return valid_rows\n\ndef write_clean_csv(rows):\n    """Write cleaned data to clean_users.csv"""\n    with open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n        if rows:\n            writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n            writer.writeheader()\n            writer.writerows(rows)\n\ndef import_to_db(rows):\n    """Import cleaned users to database"""\n    conn = psycopg2.connect(\n        dbname=\'production_db\',\n        user=\'postgres\',\n        host=\'localhost\'\n    )\n    cur = conn.cursor()\n    \n    for row in rows:\n        cur.execute(\n            "INSERT INTO users (user_id, username, email, created_at, status) VALUES (%s, %s, %s, %s, %s)",\n            (row[\'user_id\'], row[\'username\'], row[\'email\'], row[\'created_at\'], row[\'status\'])\n        )\n    \n    conn.commit()\n    cur.close()\n    conn.close()\n\nif __name__ == \'__main__\':\n    print("Cleaning users data...")\n    valid_rows = clean_users()\n    print(f"Valid rows: {len(valid_rows)}")\n    \n    print("Writing clean_users.csv...")\n    write_clean_csv(valid_rows)\n    \n    print("Importing to database...")\n    import_to_db(valid_rows)\n    \n    print("Done!")\nEOF\n'
'python3 clean_and_import_users.py\n'
'cat > clean_users.py << \'EOF\'\nimport csv\n\n# Valid status values\nVALID_STATUSES = {\'active\', \'inactive\', \'suspended\'}\n\ndef is_valid_email(email):\n    """Check if email contains @"""\n    return email and \'@\' in email\n\ndef clean_users():\n    """Clean users_export.csv and return valid rows"""\n    valid_rows = []\n    \n    with open(\'/backup/users_export.csv\', \'r\') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            # Check for missing user_id\n            if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n                continue\n            \n            # Check for empty username\n            if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n                continue\n            \n            # Check for invalid email (must contain @)\n            if not is_valid_email(row.get(\'email\', \'\')):\n                continue\n            \n            # Check for invalid status\n            if row.get(\'status\', \'\').strip().lower() not in VALID_STATUSES:\n                continue\n            \n            valid_rows.append(row)\n    \n    return valid_rows\n\ndef write_clean_csv(rows):\n    """Write cleaned data to clean_users.csv"""\n    with open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n        if rows:\n            writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n            writer.writeheader()\n            writer.writerows(rows)\n\nif __name__ == \'__main__\':\n    print("Cleaning users data...")\n    valid_rows = clean_users()\n    print(f"Valid rows: {len(valid_rows)}")\n    \n    print("Writing clean_users.csv...")\n    write_clean_csv(valid_rows)\n    \n    print("Done!")\nEOF\n'
'python3 clean_users.py\n'
'head -5 clean_users.csv\n'
'wc -l clean_users.csv\n'
'psql -U postgres -d production_db -c "\\COPY users FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);"\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) FROM users;"\n'
'cat > clean_transactions.py << \'EOF\'\nimport json\n\ndef clean_transactions():\n    """Clean transactions_backup.json and return valid transactions"""\n    with open(\'/backup/transactions_backup.json\', \'r\') as f:\n        data = json.load(f)\n    \n    valid_transactions = []\n    \n    for txn in data:\n        # Check for null or missing transaction_id\n        if txn.get(\'transaction_id\') is None:\n            continue\n        \n        # Check for null or missing user_id\n        if txn.get(\'user_id\') is None:\n            continue\n        \n        # Check if user_id is in valid range (1-30)\n        try:\n            user_id = int(txn[\'user_id\'])\n            if user_id < 1 or user_id > 30:\n                continue\n        except (ValueError, TypeError):\n            continue\n        \n        # Check for null or negative amount\n        if txn.get(\'amount\') is None:\n            continue\n        \n        try:\n            amount = float(txn[\'amount\'])\n            if amount <= 0:\n                continue\n        except (ValueError, TypeError):\n            continue\n        \n        # Check for missing timestamp\n        if not txn.get(\'timestamp\'):\n            continue\n        \n        valid_transactions.append(txn)\n    \n    return valid_transactions\n\ndef write_valid_json(transactions):\n    """Write valid transactions to valid_transactions.json"""\n    with open(\'/backup/valid_transactions.json\', \'w\') as f:\n        json.dump(transactions, f, indent=2)\n        f.write(\'\\n\')  # Ensure newline termination\n\nif __name__ == \'__main__\':\n    print("Cleaning transactions data...")\n    valid_txns = clean_transactions()\n    print(f"Valid transactions: {len(valid_txns)}")\n    \n    print("Writing valid_transactions.json...")\n    write_valid_json(valid_txns)\n    \n    print("Done!")\nEOF\n'
'python3 clean_transactions.py\n'
'cat > import_transactions.py << \'EOF\'\nimport json\nimport subprocess\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\nprint(f"Importing {len(transactions)} transactions...")\n\n# Create SQL insert statements\nsql_statements = []\nfor txn in transactions:\n    sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{txn[\'description\']}\');"\n    sql_statements.append(sql)\n\n# Write to temp SQL file\nwith open(\'/tmp/import_transactions.sql\', \'w\') as f:\n    f.write(\'\\n\'.join(sql_statements))\n\nprint("Executing SQL import...")\nresult = subprocess.run(\n    [\'psql\', \'-U\', \'postgres\', \'-d\', \'production_db\', \'-f\', \'/tmp/import_transactions.sql\'],\n    capture_output=True,\n    text=True\n)\n\nif result.returncode == 0:\n    print("Import successful!")\nelse:\n    print(f"Error: {result.stderr}")\nEOF\n'
'python3 import_transactions.py\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) FROM transactions;"\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) FROM users;"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'su - postgres -c "psql -d production_db -f /tmp/import_transactions.sql" 2>&1 | tail -20\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'su - postgres -c "psql -c \'\\l\'"\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \\"\\\\COPY users FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -f /tmp/import_transactions.sql" 2>&1 | head -5\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'wc -l /tmp/import_transactions.sql\n'
'su - postgres -c "psql -d production_db -c \'DELETE FROM transactions;\'"\n'
'head -10 /tmp/import_transactions.sql\n'
'tail -10 /tmp/import_transactions.sql\n'
'python3 << \'EOF\'\nimport json\n\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\nwith open(\'/tmp/import_transactions.sql\', \'w\') as f:\n    for txn in transactions:\n        desc = txn[\'description\'].replace("\'", "\'\'")\n        sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({txn[\'transaction_id\']}, {txn[\'user_id\']}, {txn[\'amount\']}, \'{txn[\'timestamp\']}\', \'{desc}\');\\n"\n        f.write(sql)\nprint(f"Generated SQL for {len(transactions)} transactions")\nEOF\n'
'wc -l /tmp/import_transactions.sql\n'
'su - postgres -c "psql -d production_db -f /tmp/import_transactions.sql" > /tmp/import_output.txt 2>&1\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\ndef run_query(query):\n    """Run a SQL query and return the result"""\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f"psql -d production_db -t -A -c \\"{query}\\""],\n        capture_output=True,\n        text=True,\n        shell=False\n    )\n    return result.stdout.strip()\n\n# Generate db_stats.txt\nprint("Generating database statistics...")\n\ntotal_users = run_query("SELECT COUNT(*) FROM users;")\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query("SELECT COUNT(*) FROM transactions;")\ntotal_amount = run_query("SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;")\navg_per_user = run_query("SELECT ROUND(AVG(total)::numeric, 2) FROM (SELECT user_id, SUM(amount) as total FROM transactions GROUP BY user_id) subq;")\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write("DATABASE STATISTICS\\n")\n    f.write("==================\\n")\n    f.write(f"Total Users: {total_users}\\n")\n    f.write(f"Active Users: {active_users}\\n")\n    f.write(f"Total Transactions: {total_transactions}\\n")\n    f.write(f"Total Amount: {total_amount}\\n")\n    f.write(f"Average per User: {avg_per_user}\\n")\n\nprint("db_stats.txt generated")\n\n# Generate top_users.csv\nprint("Generating top users report...")\n\ntop_users_query = """\nSELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n"""\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -c "{top_users_query}" -A -F, --no-align\'],\n    capture_output=True,\n    text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write("username,total_amount\\n")\n    lines = result.stdout.strip().split(\'\\n\')\n    # Skip header and row count lines\n    for line in lines:\n        if line and not line.startswith(\'(\') and \'|\' not in line and line != \'username|total_amount\':\n            f.write(line + \'\\n\')\n\nprint("top_users.csv generated")\n\n# Generate backup_verification.txt\nprint("Generating backup verification...")\n\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    log_content = f.read()\n\nbackup_date = \'\'\ntables_backed_up = \'\'\ntotal_rows = \'\'\nintegrity_check = \'\'\n\nfor line in log_content.split(\'\\n\'):\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write("BACKUP VERIFICATION\\n")\n    f.write("===================\\n")\n    f.write(f"Backup Date: {backup_date}\\n")\n    f.write(f"Tables Backed Up: {tables_backed_up}\\n")\n    f.write(f"Total Rows: {total_rows}\\n")\n    f.write(f"Integrity Check: {integrity_check}\\n")\n\nprint("backup_verification.txt generated")\n\n# Create recovery completion marker\nwith open(\'/backup/recovery_complete.txt\', \'w\') as f:\n    f.write("RECOVERY_SUCCESSFUL\\n")\n\nprint("recovery_complete.txt created")\nprint("All reports generated successfully!")\nEOF\n'
'python3 generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'cat /backup/backup_verification.txt\n'
'cat /backup/recovery_complete.txt\n'
'wc -l /backup/*.csv /backup/*.txt /backup/*.json\n'
'su - postgres -c "psql -d production_db -t -A -F, -c \\"SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10;\\"" > /tmp/top_users_data.csv\n'
'cat /tmp/top_users_data.csv\n'
"echo 'username,total_amount' > /backup/top_users.csv && cat /tmp/top_users_data.csv >> /backup/top_users.csv\n"
'cat /backup/top_users.csv\n'
'wc -l /backup/top_users.csv\n'
"ls -la /backup/*.csv /backup/*.txt /backup/*.json | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification|recovery_complete)'\n"
"echo '=== Verifying clean_users.csv ===' && head -3 /backup/clean_users.csv && tail -2 /backup/clean_users.csv\n"
'echo \'=== Verifying valid_transactions.json ===\' && python3 -c "import json; d=json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid JSON: {len(d)} transactions\'); print(\'First:\', d[0]); print(\'Last:\', d[-1])"\n'
'echo \'=== Database verification ===\' && su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as users FROM users; SELECT COUNT(*) as transactions FROM transactions;\'"\n'
"echo '=== All required files ===' && ls -1 /backup/ | grep -E '(clean_users.csv|valid_transactions.json|db_stats.txt|top_users.csv|backup_verification.txt|recovery_complete.txt)$'\n"
'echo \'=== Checking newline termination ===\' && for f in clean_users.csv valid_transactions.json db_stats.txt top_users.csv backup_verification.txt recovery_complete.txt; do echo -n "$f: "; tail -c 1 /backup/$f | od -An -tx1; done\n'
"echo '=== Final content verification ===' && echo 'db_stats.txt:' && cat /backup/db_stats.txt && echo '' && echo 'backup_verification.txt:' && cat /backup/backup_verification.txt\n"
'echo \'=== Final summary ===\' && echo \'Users in DB:\' && su - postgres -c "psql -d production_db -t -c \'SELECT COUNT(*) FROM users;\'" && echo \'Transactions in DB:\' && su - postgres -c "psql -d production_db -t -c \'SELECT COUNT(*) FROM transactions;\'" && echo \'Files created:\' && ls -1 /backup/ | grep -E \'(clean_users.csv|valid_transactions.json|db_stats.txt|top_users.csv|backup_verification.txt|recovery_complete.txt)$\' | wc -l\n'
"echo 'Task completed successfully!' && echo 'Summary:' && echo '- Database: production_db with users and transactions tables' && echo '- Users imported: 30 (cleaned from 36 invalid rows)' && echo '- Transactions imported: 100 (cleaned from 106 invalid entries)' && echo '- Reports generated: db_stats.txt, top_users.csv, backup_verification.txt' && echo '- Recovery marker: recovery_complete.txt'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
