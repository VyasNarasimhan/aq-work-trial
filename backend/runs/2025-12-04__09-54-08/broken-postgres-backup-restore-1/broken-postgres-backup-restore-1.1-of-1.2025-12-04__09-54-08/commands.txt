['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
"psql -U postgres -d production_db -c '\\d users'\n"
"psql -U postgres -d production_db -c '\\d transactions'\n"
'wc -l /backup/users_export.csv /backup/transactions_backup.json\n'
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d users\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d transactions\'"\n'
'cat > /backup/clean_data.py << \'EOF\'\nimport csv\nimport json\nimport re\n\n# Clean users CSV\nprint("Cleaning users data...")\nvalid_users = []\nvalid_statuses = {\'active\', \'inactive\', \'suspended\'}\nemail_pattern = re.compile(r\'.+@.+\')\n\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check if user_id exists and is not empty\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        # Check if username is not empty\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        # Check if email is valid (contains @)\n        if not row.get(\'email\') or not email_pattern.match(row[\'email\']):\n            continue\n        # Check if status is valid\n        if not row.get(\'status\') or row[\'status\'].strip() not in valid_statuses:\n            continue\n        valid_users.append(row)\n\nprint(f"Valid users: {len(valid_users)}")\n\n# Write cleaned users to CSV\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint("Users cleaned and saved to clean_users.csv")\n\n# Clean transactions JSON\nprint("\\nCleaning transactions data...")\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check transaction_id is not null\n    if txn.get(\'transaction_id\') is None:\n        continue\n    # Check amount is not null and is positive\n    if txn.get(\'amount\') is None or txn[\'amount\'] <= 0:\n        continue\n    # Check timestamp exists\n    if not txn.get(\'timestamp\'):\n        continue\n    # Check user_id is valid (1-30)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    valid_transactions.append(txn)\n\nprint(f"Valid transactions: {len(valid_transactions)}")\n\n# Write valid transactions to JSON\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint("Transactions cleaned and saved to valid_transactions.json")\nEOF\n'
'python3 /backup/clean_data.py\n'
'cat > /backup/clean_data.py << \'EOF\'\nimport csv\nimport json\nimport re\n\n# Clean users CSV\nprint("Cleaning users data...")\nvalid_users = []\nvalid_statuses = {\'active\', \'inactive\', \'suspended\'}\nemail_pattern = re.compile(r\'.+@.+\')\n\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check if user_id exists and is not empty\n        if not row.get(\'user_id\') or not row[\'user_id\'].strip():\n            continue\n        # Check if username is not empty\n        if not row.get(\'username\') or not row[\'username\'].strip():\n            continue\n        # Check if email is valid (contains @)\n        if not row.get(\'email\') or not email_pattern.match(row[\'email\']):\n            continue\n        # Check if status is valid\n        if not row.get(\'status\') or row[\'status\'].strip() not in valid_statuses:\n            continue\n        valid_users.append(row)\n\nprint(f"Valid users: {len(valid_users)}")\n\n# Write cleaned users to CSV\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    if valid_users:\n        writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n        writer.writeheader()\n        writer.writerows(valid_users)\n\nprint("Users cleaned and saved to clean_users.csv")\n\n# Clean transactions JSON\nprint("\\nCleaning transactions data...")\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor txn in transactions:\n    # Check transaction_id is not null\n    if txn.get(\'transaction_id\') is None:\n        continue\n    # Check timestamp exists\n    if not txn.get(\'timestamp\'):\n        continue\n    # Check user_id is valid (1-30)\n    if txn.get(\'user_id\') is None or txn[\'user_id\'] < 1 or txn[\'user_id\'] > 30:\n        continue\n    # Check amount is not null and is positive\n    amount = txn.get(\'amount\')\n    if amount is None:\n        continue\n    try:\n        amount_val = float(amount)\n        if amount_val <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(txn)\n\nprint(f"Valid transactions: {len(valid_transactions)}")\n\n# Write valid transactions to JSON\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint("Transactions cleaned and saved to valid_transactions.json")\nEOF\n'
'python3 /backup/clean_data.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Load valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Insert transactions\nfor txn in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn.get(\'description\', \'\'))\n    )\n\nconn.commit()\nprint(f"Imported {len(transactions)} transactions")\n\ncur.close()\nconn.close()\nEOF\n'
'python3 /backup/import_transactions.py\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /backup/json_to_csv.py << \'EOF\'\nimport json\nimport csv\n\n# Load valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Write to CSV\nwith open(\'/backup/transactions.csv\', \'w\', newline=\'\') as f:\n    writer = csv.writer(f)\n    for txn in transactions:\n        writer.writerow([\n            txn[\'transaction_id\'],\n            txn[\'user_id\'],\n            txn[\'amount\'],\n            txn[\'timestamp\'],\n            txn.get(\'description\', \'\')\n        ])\n\nprint(f"Converted {len(transactions)} transactions to CSV")\nEOF\n'
'python3 /backup/json_to_csv.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY transactions(transaction_id, user_id, amount, timestamp, description) FROM \'/backup/transactions.csv\' WITH (FORMAT csv);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /backup/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -c "{query}"\'],\n        capture_output=True,\n        text=True\n    )\n    return result.stdout.strip()\n\n# Generate db_stats.txt\nprint("Generating database statistics...")\ntotal_users = run_query(\'SELECT COUNT(*) FROM users;\')\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query(\'SELECT COUNT(*) FROM transactions;\')\ntotal_amount = run_query(\'SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;\')\navg_per_user = run_query(\'SELECT ROUND(AVG(total)::numeric, 2) FROM (SELECT SUM(amount) as total FROM transactions GROUP BY user_id) as user_totals;\')\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount}\\n\')\n    f.write(f\'Average per User: {avg_per_user}\\n\')\n\nprint("db_stats.txt created")\n\n# Generate top_users.csv\nprint("\\nGenerating top users report...")\nquery = """SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount \n           FROM users u \n           JOIN transactions t ON u.user_id = t.user_id \n           GROUP BY u.username \n           ORDER BY total_amount DESC, u.username ASC \n           LIMIT 10;"""\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -F, -c "{query}"\'],\n    capture_output=True,\n    text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(\'username,total_amount\\n\')\n    f.write(result.stdout)\n\nprint("top_users.csv created")\n\n# Generate backup_verification.txt\nprint("\\nGenerating backup verification...")\nwith open(\'/backup/backup_metadata.log\', \'r\') as f:\n    lines = f.readlines()\n\nbackup_date = \'\'\ntables_backed_up = \'\'\ntotal_rows = \'\'\nintegrity_check = \'\'\n\nfor line in lines:\n    if \'Backup timestamp:\' in line:\n        backup_date = line.split(\'Backup timestamp:\')[1].strip()\n    elif \'Total tables backed up:\' in line:\n        tables_backed_up = line.split(\'Total tables backed up:\')[1].strip()\n    elif \'Total rows backed up:\' in line:\n        total_rows = line.split(\'Total rows backed up:\')[1].strip()\n    elif \'Integrity check:\' in line:\n        integrity_check = line.split(\'Integrity check:\')[1].strip()\n\nwith open(\'/backup/backup_verification.txt\', \'w\') as f:\n    f.write(\'BACKUP VERIFICATION\\n\')\n    f.write(\'===================\\n\')\n    f.write(f\'Backup Date: {backup_date}\\n\')\n    f.write(f\'Tables Backed Up: {tables_backed_up}\\n\')\n    f.write(f\'Total Rows: {total_rows}\\n\')\n    f.write(f\'Integrity Check: {integrity_check}\\n\')\n\nprint("backup_verification.txt created")\n\nprint("\\nAll reports generated successfully!")\nEOF\n'
'python3 /backup/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'cat /backup/backup_verification.txt\n'
'wc -l /backup/clean_users.csv /backup/valid_transactions.json /backup/top_users.csv\n'
'head -3 /backup/clean_users.csv\n'
'tail -3 /backup/valid_transactions.json\n'
"echo 'RECOVERY_SUCCESSFUL' > /backup/recovery_complete.txt\n"
"ls -la /backup/*.txt /backup/*.csv /backup/*.json 2>/dev/null | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification|recovery_complete)'\n"
'cat /backup/recovery_complete.txt\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as txn_count FROM transactions;\'"\n'
'for file in /backup/clean_users.csv /backup/valid_transactions.json /backup/db_stats.txt /backup/top_users.csv /backup/backup_verification.txt /backup/recovery_complete.txt; do echo "Checking $file:"; tail -c 1 "$file" | od -An -tx1; done\n'
'python3 -c "import json; json.load(open(\'/backup/valid_transactions.json\')); print(\'valid_transactions.json is valid JSON\')"\n'
'head -1 /backup/top_users.csv && tail -1 /backup/top_users.csv\n'
"echo 'All tasks completed successfully!'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
