['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
"psql -U postgres -d production_db -c '\\d users'\n"
"psql -U postgres -d production_db -c '\\d transactions'\n"
'wc -l /backup/users_export.csv\n'
'cat /backup/users_export.csv\n'
'cat /backup/transactions_backup.json | python3 -m json.tool | head -100\n'
'python3 -c "import json; data=json.load(open(\'/backup/transactions_backup.json\')); print(f\'Total transactions: {len(data)}\'); print(\'Sample invalid entries:\'); [print(t) for t in data if t.get(\'transaction_id\') is None or t.get(\'amount\',0) <= 0 or t.get(\'user_id\',0) < 1 or t.get(\'user_id\',0) > 30][:10]"\n'
'cat > /backup/clean_data.py << \'EOF\'\nimport csv\nimport json\nimport re\n\n# Clean users CSV\nprint("Cleaning users data...")\nvalid_users = []\nwith open(\'/backup/users_export.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        # Check for missing user_id\n        if not row.get(\'user_id\') or row[\'user_id\'].strip() == \'\':\n            continue\n        \n        # Check for empty username\n        if not row.get(\'username\') or row[\'username\'].strip() == \'\':\n            continue\n        \n        # Check for valid email (must contain @)\n        email = row.get(\'email\', \'\').strip()\n        if not email or \'@\' not in email:\n            continue\n        \n        # Check for valid status\n        status = row.get(\'status\', \'\').strip()\n        if status not in [\'active\', \'inactive\', \'suspended\']:\n            continue\n        \n        valid_users.append(row)\n\nprint(f"Valid users: {len(valid_users)}")\n\n# Write cleaned users\nwith open(\'/backup/clean_users.csv\', \'w\', newline=\'\') as f:\n    writer = csv.DictWriter(f, fieldnames=[\'user_id\', \'username\', \'email\', \'created_at\', \'status\'])\n    writer.writeheader()\n    writer.writerows(valid_users)\n\nprint("Users cleaned and saved to clean_users.csv")\n\n# Clean transactions JSON\nprint("\\nCleaning transactions data...")\nwith open(\'/backup/transactions_backup.json\', \'r\') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\nfor t in transactions:\n    # Check for null transaction_id\n    if t.get(\'transaction_id\') is None:\n        continue\n    \n    # Check for null or invalid user_id (must be 1-30)\n    user_id = t.get(\'user_id\')\n    if user_id is None or not isinstance(user_id, int) or user_id < 1 or user_id > 30:\n        continue\n    \n    # Check for null, negative, or zero amounts\n    amount = t.get(\'amount\')\n    if amount is None:\n        continue\n    \n    # Convert string amounts to float if needed\n    try:\n        amount_val = float(amount)\n    except (ValueError, TypeError):\n        continue\n    \n    if amount_val <= 0:\n        continue\n    \n    # Check for missing timestamp\n    if not t.get(\'timestamp\'):\n        continue\n    \n    valid_transactions.append(t)\n\nprint(f"Valid transactions: {len(valid_transactions)}")\n\n# Write valid transactions\nwith open(\'/backup/valid_transactions.json\', \'w\') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write(\'\\n\')\n\nprint("Transactions cleaned and saved to valid_transactions.json")\nEOF\n'
'python3 /backup/clean_data.py\n'
'psql -U postgres -d production_db -c "COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);"\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) FROM users;"\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'localhost\'\n)\ncur = conn.cursor()\n\n# Load transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\nprint(f"Importing {len(transactions)} transactions...")\n\n# Insert transactions\nfor t in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (t[\'transaction_id\'], t[\'user_id\'], t[\'amount\'], t[\'timestamp\'], t[\'description\'])\n    )\n\nconn.commit()\nprint("Transactions imported successfully")\n\ncur.close()\nconn.close()\nEOF\n'
'python3 /backup/import_transactions.py\n'
'psql -U postgres -d production_db -c "SELECT COUNT(*) FROM transactions;"\n'
'su - postgres -c "psql -d production_db -c \'COPY users(user_id, username, email, created_at, status) FROM \\\'/backup/clean_users.csv\\\' WITH (FORMAT csv, HEADER true);\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /backup/convert_json_to_csv.py << \'EOF\'\nimport json\nimport csv\n\n# Load transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Write to CSV\nwith open(\'/backup/transactions_temp.csv\', \'w\', newline=\'\') as f:\n    writer = csv.writer(f)\n    for t in transactions:\n        writer.writerow([t[\'transaction_id\'], t[\'user_id\'], t[\'amount\'], t[\'timestamp\'], t[\'description\']])\n\nprint(f"Converted {len(transactions)} transactions to CSV")\nEOF\n'
'python3 /backup/convert_json_to_csv.py\n'
'su - postgres -c "psql -d production_db -c \\"COPY transactions(transaction_id, user_id, amount, timestamp, description) FROM \'/backup/transactions_temp.csv\' WITH (FORMAT csv);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'su - postgres\n'
"psql -c 'CREATE DATABASE production_db;'\n"
'psql -d production_db -f /backup/database.sql\n'
