['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -20 /backup/users_export.csv\n'
'head -20 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d users\'"\n'
'su - postgres -c "psql -d production_db -c \'\\d transactions\'"\n'
'wc -l /backup/users_export.csv\n'
'python3 -c "import json; data=open(\'/backup/transactions_backup.json\').read(); print(f\'Total transactions: {len(json.loads(data))}\')"\n'
"cat > /tmp/clean_users.py << 'EOF'\nimport csv\nimport re\n\n# Valid status values\nvalid_statuses = {'active', 'inactive', 'suspended'}\n\n# Email validation regex\nemail_pattern = re.compile(r'.+@.+')\n\nclean_rows = []\n\nwith open('/backup/users_export.csv', 'r') as f:\n    reader = csv.DictReader(f)\n    header = reader.fieldnames\n    \n    for row in reader:\n        # Check if user_id exists and is not empty\n        if not row.get('user_id') or not row['user_id'].strip():\n            continue\n        \n        # Check if username exists and is not empty\n        if not row.get('username') or not row['username'].strip():\n            continue\n        \n        # Check if email exists and contains @\n        if not row.get('email') or not email_pattern.match(row['email']):\n            continue\n        \n        # Check if status is valid\n        if not row.get('status') or row['status'].strip() not in valid_statuses:\n            continue\n        \n        clean_rows.append(row)\n\n# Write cleaned data\nwith open('/backup/clean_users.csv', 'w', newline='') as f:\n    writer = csv.DictWriter(f, fieldnames=header)\n    writer.writeheader()\n    writer.writerows(clean_rows)\n\nprint(f'Cleaned {len(clean_rows)} valid users out of original data')\nEOF\n"
'python3 /tmp/clean_users.py\n'
'head -10 /backup/clean_users.csv\n'
'wc -l /backup/clean_users.csv\n'
"cat > /tmp/clean_transactions.py << 'EOF'\nimport json\n\n# Load transactions\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor tx in transactions:\n    # Check transaction_id is not null\n    if tx.get('transaction_id') is None:\n        continue\n    \n    # Check amount is not null and is positive\n    if tx.get('amount') is None or tx['amount'] <= 0:\n        continue\n    \n    # Check timestamp exists\n    if not tx.get('timestamp'):\n        continue\n    \n    # Check user_id is valid (1-30)\n    if tx.get('user_id') is None or tx['user_id'] < 1 or tx['user_id'] > 30:\n        continue\n    \n    valid_transactions.append(tx)\n\n# Write valid transactions\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Cleaned {len(valid_transactions)} valid transactions out of {len(transactions)} total')\nEOF\n"
'python3 /tmp/clean_transactions.py\n'
'head -30 /backup/valid_transactions.json\n'
'python3 -c "import json; data=json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid transactions: {len(data)}\')"\n'
"cat > /tmp/clean_transactions.py << 'EOF'\nimport json\n\n# Load transactions\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor tx in transactions:\n    # Check transaction_id is not null\n    if tx.get('transaction_id') is None:\n        continue\n    \n    # Check amount is not null and is positive\n    amount = tx.get('amount')\n    if amount is None:\n        continue\n    \n    # Try to convert amount to float and check if positive\n    try:\n        amount_val = float(amount)\n        if amount_val <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check timestamp exists\n    if not tx.get('timestamp'):\n        continue\n    \n    # Check user_id is valid (1-30)\n    user_id = tx.get('user_id')\n    if user_id is None:\n        continue\n    \n    try:\n        user_id_val = int(user_id)\n        if user_id_val < 1 or user_id_val > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    valid_transactions.append(tx)\n\n# Write valid transactions\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\n\nprint(f'Cleaned {len(valid_transactions)} valid transactions out of {len(transactions)} total')\nEOF\n"
'python3 /tmp/clean_transactions.py\n'
'python3 -c "import json; data=json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid transactions: {len(data)}\')"\n'
'head -30 /backup/valid_transactions.json\n'
'cat > /tmp/import_data.py << \'EOF\'\nimport psycopg2\nimport csv\nimport json\n\n# Connect to database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Import users\nwith open(\'/backup/clean_users.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    for row in reader:\n        cur.execute(\n            "INSERT INTO users (user_id, username, email, created_at, status) VALUES (%s, %s, %s, %s, %s)",\n            (row[\'user_id\'], row[\'username\'], row[\'email\'], row[\'created_at\'], row[\'status\'])\n        )\n\nprint(f"Imported {cur.rowcount} users")\n\n# Import transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n    for tx in transactions:\n        cur.execute(\n            "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n            (tx[\'transaction_id\'], tx[\'user_id\'], tx[\'amount\'], tx[\'timestamp\'], tx[\'description\'])\n        )\n\nprint(f"Imported {len(transactions)} transactions")\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint("Data import completed successfully")\nEOF\n'
'su - postgres -c "cd /backup && python3 /tmp/import_data.py"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /tmp/import_users.py << \'EOF\'\nimport csv\n\n# Generate SQL INSERT statements for users\nwith open(\'/backup/clean_users.csv\', \'r\') as f:\n    reader = csv.DictReader(f)\n    with open(\'/tmp/users_import.sql\', \'w\') as out:\n        for row in reader:\n            sql = f"INSERT INTO users (user_id, username, email, created_at, status) VALUES ({row[\'user_id\']}, \'{row[\'username\']}\', \'{row[\'email\']}\', \'{row[\'created_at\']}\', \'{row[\'status\']}\');\\n"\n            out.write(sql)\n\nprint("Generated users import SQL")\nEOF\n'
'python3 /tmp/import_users.py\n'
'cat > /tmp/import_transactions.py << \'EOF\'\nimport json\n\n# Generate SQL INSERT statements for transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n    with open(\'/tmp/transactions_import.sql\', \'w\') as out:\n        for tx in transactions:\n            desc = tx[\'description\'].replace("\'", "\'\'")\n            sql = f"INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES ({tx[\'transaction_id\']}, {tx[\'user_id\']}, {tx[\'amount\']}, \'{tx[\'timestamp\']}\', \'{desc}\');\\n"\n            out.write(sql)\n\nprint(f"Generated {len(transactions)} transaction import SQL statements")\nEOF\n'
'python3 /tmp/import_transactions.py\n'
'su - postgres -c "psql -d production_db -f /tmp/users_import.sql"\n'
'su - postgres -c "psql -d production_db -f /tmp/transactions_import.sql"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as active_users FROM users WHERE status = \'\\\'\'active\'\\\'\';\'"\n'
'cat > /tmp/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\n# Function to run psql query\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f"psql -d production_db -t -A -c \'{query}\'"],\n        capture_output=True, text=True\n    )\n    return result.stdout.strip()\n\n# Generate db_stats.txt\ntotal_users = run_query(\'SELECT COUNT(*) FROM users;\')\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query(\'SELECT COUNT(*) FROM transactions;\')\ntotal_amount = run_query(\'SELECT ROUND(SUM(amount), 2) FROM transactions;\')\navg_per_user = run_query(\'SELECT ROUND(SUM(amount) / COUNT(DISTINCT user_id), 2) FROM transactions;\')\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount}\\n\')\n    f.write(f\'Average per User: {avg_per_user}\\n\')\n\nprint(\'Generated db_stats.txt\')\n\n# Generate top_users.csv\ntop_users_query = \'\'\'\nSELECT u.username, ROUND(SUM(t.amount), 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n\'\'\'\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f"psql -d production_db -t -A -F\',\' -c \\"{top_users_query}\\""],\n    capture_output=True, text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(\'username,total_amount\\n\')\n    f.write(result.stdout)\n\nprint(\'Generated top_users.csv\')\nEOF\n'
'python3 /tmp/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
'su - postgres -c "psql -d production_db -t -A -c \\"SELECT COUNT(*) FROM users WHERE status = \'active\';\\""\n'
'cat > /tmp/fix_reports.py << \'EOF\'\nimport subprocess\n\n# Fix db_stats.txt with correct active users count\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -c "{query}"\'],\n        capture_output=True, text=True\n    )\n    return result.stdout.strip()\n\ntotal_users = run_query(\'SELECT COUNT(*) FROM users;\')\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query(\'SELECT COUNT(*) FROM transactions;\')\ntotal_amount = run_query(\'SELECT ROUND(SUM(amount), 2) FROM transactions;\')\navg_per_user = run_query(\'SELECT ROUND(SUM(amount) / COUNT(DISTINCT user_id), 2) FROM transactions;\')\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount}\\n\')\n    f.write(f\'Average per User: {avg_per_user}\\n\')\n\nprint(\'Fixed db_stats.txt\')\nEOF\n'
'python3 /tmp/fix_reports.py\n'
'cat /backup/db_stats.txt\n'
"cat > /tmp/backup_verification.py << 'EOF'\n# Parse backup_metadata.log and create backup_verification.txt\nwith open('/backup/backup_metadata.log', 'r') as f:\n    lines = f.readlines()\n\nbackup_date = ''\ntables_backed_up = ''\ntotal_rows = ''\nintegrity_check = ''\n\nfor line in lines:\n    if 'Backup timestamp:' in line:\n        backup_date = line.split('Backup timestamp:')[1].strip()\n    elif 'Total tables backed up:' in line:\n        tables_backed_up = line.split('Total tables backed up:')[1].strip()\n    elif 'Total rows backed up:' in line:\n        total_rows = line.split('Total rows backed up:')[1].strip()\n    elif 'Integrity check:' in line:\n        integrity_check = line.split('Integrity check:')[1].strip()\n\nwith open('/backup/backup_verification.txt', 'w') as f:\n    f.write('BACKUP VERIFICATION\\n')\n    f.write('===================\\n')\n    f.write(f'Backup Date: {backup_date}\\n')\n    f.write(f'Tables Backed Up: {tables_backed_up}\\n')\n    f.write(f'Total Rows: {total_rows}\\n')\n    f.write(f'Integrity Check: {integrity_check}\\n')\n\nprint('Generated backup_verification.txt')\nEOF\n"
'python3 /tmp/backup_verification.py\n'
'cat /backup/backup_verification.txt\n'
"echo 'RECOVERY_SUCCESSFUL' > /backup/recovery_complete.txt\n"
'ls -la /backup/*.txt /backup/*.csv /backup/*.json\n'
'tail -c 20 /backup/clean_users.csv | od -c\n'
'tail -c 20 /backup/valid_transactions.json | od -c\n'
'tail -c 20 /backup/db_stats.txt | od -c\n'
'tail -c 20 /backup/top_users.csv | od -c\n'
'tail -c 20 /backup/backup_verification.txt | od -c\n'
'tail -c 20 /backup/recovery_complete.txt | od -c\n'
'wc -l /backup/top_users.csv\n'
'su - postgres -c "psql -d production_db -c \'SELECT username, ROUND(SUM(amount), 2) as total FROM transactions t JOIN users u ON t.user_id = u.user_id GROUP BY username ORDER BY total DESC, username ASC LIMIT 10;\'"\n'
'cat /backup/top_users.csv\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as users FROM users; SELECT COUNT(*) as transactions FROM transactions;\'"\n'
"ls -1 /backup/ | grep -E '(clean_users.csv|valid_transactions.json|db_stats.txt|top_users.csv|backup_verification.txt|recovery_complete.txt)'\n"
"echo 'All tasks completed successfully!'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
