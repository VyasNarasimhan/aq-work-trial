['asciinema rec --stdin /logs/agent.cast', 'Enter']
['clear', 'Enter']
'ls -la /backup/\n'
'service postgresql start\n'
'head -20 /backup/database.sql\n'
'head -10 /backup/users_export.csv\n'
'head -10 /backup/transactions_backup.json\n'
'cat /backup/backup_metadata.log\n'
"psql -U postgres -c 'CREATE DATABASE production_db;'\n"
'psql -U postgres -d production_db -f /backup/database.sql\n'
"psql -U postgres -d production_db -c '\\dt'\n"
'su - postgres -c "psql -c \'CREATE DATABASE production_db;\'"\n'
'su - postgres -c "psql -d production_db -f /backup/database.sql"\n'
'su - postgres -c "psql -d production_db -c \'\\dt\'"\n'
"cat > /backup/clean_users.py << 'EOF'\nimport csv\nimport re\n\n# Valid status values\nvalid_statuses = {'active', 'inactive', 'suspended'}\n\n# Email validation regex\nemail_pattern = re.compile(r'^[^@]+@[^@]+$')\n\nwith open('/backup/users_export.csv', 'r') as infile, open('/backup/clean_users.csv', 'w', newline='') as outfile:\n    reader = csv.DictReader(infile)\n    writer = csv.DictWriter(outfile, fieldnames=['user_id', 'username', 'email', 'created_at', 'status'])\n    writer.writeheader()\n    \n    for row in reader:\n        # Check for missing or empty user_id\n        if not row.get('user_id', '').strip():\n            continue\n        \n        # Check for empty username\n        if not row.get('username', '').strip():\n            continue\n        \n        # Check for invalid email (must contain @)\n        email = row.get('email', '').strip()\n        if not email or '@' not in email:\n            continue\n        \n        # Check for invalid status\n        status = row.get('status', '').strip()\n        if status not in valid_statuses:\n            continue\n        \n        # Write valid row\n        writer.writerow({\n            'user_id': row['user_id'].strip(),\n            'username': row['username'].strip(),\n            'email': email,\n            'created_at': row.get('created_at', '').strip(),\n            'status': status\n        })\nEOF\n"
'python3 /backup/clean_users.py\n'
'head -15 /backup/clean_users.csv\n'
'wc -l /backup/clean_users.csv\n'
'su - postgres -c "psql -d production_db -c \\"COPY users(user_id, username, email, created_at, status) FROM \'/backup/clean_users.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
"cat > /backup/clean_transactions.py << 'EOF'\nimport json\n\n# Read the transactions backup file\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    \n    # Check for null or negative amounts\n    amount = txn.get('amount')\n    if amount is None or amount <= 0:\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    \n    # Check for invalid user_id (must be 1-30)\n    user_id = txn.get('user_id')\n    if user_id is None or user_id < 1 or user_id > 30:\n        continue\n    \n    # Transaction is valid\n    valid_transactions.append(txn)\n\n# Write valid transactions to output file\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\nEOF\n"
'python3 /backup/clean_transactions.py\n'
'wc -l /backup/valid_transactions.json\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
"cat > /backup/clean_transactions.py << 'EOF'\nimport json\n\n# Read the transactions backup file\nwith open('/backup/transactions_backup.json', 'r') as f:\n    transactions = json.load(f)\n\nvalid_transactions = []\n\nfor txn in transactions:\n    # Check for null transaction_id\n    if txn.get('transaction_id') is None:\n        continue\n    \n    # Check for null or negative amounts\n    amount = txn.get('amount')\n    if amount is None:\n        continue\n    \n    # Convert amount to float if it's a string\n    try:\n        amount_val = float(amount)\n        if amount_val <= 0:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Check for missing timestamp\n    if not txn.get('timestamp'):\n        continue\n    \n    # Check for invalid user_id (must be 1-30)\n    user_id = txn.get('user_id')\n    if user_id is None:\n        continue\n    \n    try:\n        user_id_val = int(user_id)\n        if user_id_val < 1 or user_id_val > 30:\n            continue\n    except (ValueError, TypeError):\n        continue\n    \n    # Transaction is valid - keep original data\n    valid_transactions.append(txn)\n\n# Write valid transactions to output file\nwith open('/backup/valid_transactions.json', 'w') as f:\n    json.dump(valid_transactions, f, indent=2)\n    f.write('\\n')\nEOF\n"
'python3 /backup/clean_transactions.py\n'
'head -30 /backup/valid_transactions.json\n'
'python3 -c "import json; data = json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid transactions: {len(data)}\')"\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM users;\'"\n'
'cat > /backup/import_transactions.py << \'EOF\'\nimport json\nimport psycopg2\n\n# Connect to the database\nconn = psycopg2.connect(\n    dbname=\'production_db\',\n    user=\'postgres\',\n    host=\'/var/run/postgresql\'\n)\ncur = conn.cursor()\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Insert transactions\nfor txn in transactions:\n    cur.execute(\n        "INSERT INTO transactions (transaction_id, user_id, amount, timestamp, description) VALUES (%s, %s, %s, %s, %s)",\n        (txn[\'transaction_id\'], txn[\'user_id\'], txn[\'amount\'], txn[\'timestamp\'], txn[\'description\'])\n    )\n\nconn.commit()\ncur.close()\nconn.close()\n\nprint(f"Imported {len(transactions)} transactions")\nEOF\n'
'python3 /backup/import_transactions.py\n'
'cat > /backup/json_to_csv.py << \'EOF\'\nimport json\nimport csv\n\n# Read valid transactions\nwith open(\'/backup/valid_transactions.json\', \'r\') as f:\n    transactions = json.load(f)\n\n# Write to CSV\nwith open(\'/backup/transactions.csv\', \'w\', newline=\'\') as f:\n    writer = csv.writer(f)\n    writer.writerow([\'transaction_id\', \'user_id\', \'amount\', \'timestamp\', \'description\'])\n    for txn in transactions:\n        writer.writerow([\n            txn[\'transaction_id\'],\n            txn[\'user_id\'],\n            txn[\'amount\'],\n            txn[\'timestamp\'],\n            txn[\'description\']\n        ])\n\nprint(f"Converted {len(transactions)} transactions to CSV")\nEOF\n'
'python3 /backup/json_to_csv.py\n'
'head -10 /backup/transactions.csv\n'
'su - postgres -c "psql -d production_db -c \\"COPY transactions(transaction_id, user_id, amount, timestamp, description) FROM \'/backup/transactions.csv\' WITH (FORMAT csv, HEADER true);\\""\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) FROM transactions;\'"\n'
'cat > /backup/generate_reports.py << \'EOF\'\nimport subprocess\nimport json\n\ndef run_query(query):\n    result = subprocess.run(\n        [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -c "{query}"\'],\n        capture_output=True,\n        text=True\n    )\n    return result.stdout.strip()\n\n# Generate db_stats.txt\ntotal_users = run_query(\'SELECT COUNT(*) FROM users;\')\nactive_users = run_query("SELECT COUNT(*) FROM users WHERE status = \'active\';")\ntotal_transactions = run_query(\'SELECT COUNT(*) FROM transactions;\')\ntotal_amount = run_query(\'SELECT ROUND(SUM(amount)::numeric, 2) FROM transactions;\')\navg_per_user = run_query(\'SELECT ROUND((SUM(amount) / COUNT(DISTINCT user_id))::numeric, 2) FROM transactions;\')\n\nwith open(\'/backup/db_stats.txt\', \'w\') as f:\n    f.write(\'DATABASE STATISTICS\\n\')\n    f.write(\'==================\\n\')\n    f.write(f\'Total Users: {total_users}\\n\')\n    f.write(f\'Active Users: {active_users}\\n\')\n    f.write(f\'Total Transactions: {total_transactions}\\n\')\n    f.write(f\'Total Amount: {total_amount}\\n\')\n    f.write(f\'Average per User: {avg_per_user}\\n\')\n\nprint(\'Generated db_stats.txt\')\n\n# Generate top_users.csv\ntop_users_query = \'\'\'\nSELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n\'\'\'\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -c "COPY ({top_users_query.strip()}) TO STDOUT WITH CSV HEADER;"\'],\n    capture_output=True,\n    text=True\n)\n\nwith open(\'/backup/top_users.csv\', \'w\') as f:\n    f.write(result.stdout)\n\nprint(\'Generated top_users.csv\')\nEOF\n'
'python3 /backup/generate_reports.py\n'
'cat /backup/db_stats.txt\n'
'cat /backup/top_users.csv\n'
"cat > /backup/backup_verification.txt << 'EOF'\nBACKUP VERIFICATION\n===================\nBackup Date: 2025-09-28 03:00:00\nTables Backed Up: 2\nTotal Rows: 130\nIntegrity Check: PASSED\nEOF\n"
'cat /backup/backup_verification.txt\n'
"echo 'RECOVERY_SUCCESSFUL' > /backup/recovery_complete.txt\n"
'su - postgres -c "psql -d production_db -c \\"COPY (SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10) TO \'/backup/top_users.csv\' WITH CSV HEADER;\\""\n'
'cat /backup/top_users.csv\n'
'wc -l /backup/top_users.csv\n'
'su - postgres -c "psql -d production_db -c \\"\\\\copy (SELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount FROM users u JOIN transactions t ON u.user_id = t.user_id GROUP BY u.username ORDER BY total_amount DESC, u.username ASC LIMIT 10) TO \'/backup/top_users.csv\' WITH CSV HEADER\\""\n'
'cat /backup/top_users.csv\n'
'wc -l /backup/top_users.csv\n'
'cat > /backup/generate_top_users.py << \'EOF\'\nimport subprocess\nimport csv\n\n# Query to get top 10 users\nquery = \'\'\'\nSELECT u.username, ROUND(SUM(t.amount)::numeric, 2) as total_amount\nFROM users u\nJOIN transactions t ON u.user_id = t.user_id\nGROUP BY u.username\nORDER BY total_amount DESC, u.username ASC\nLIMIT 10;\n\'\'\'\n\nresult = subprocess.run(\n    [\'su\', \'-\', \'postgres\', \'-c\', f\'psql -d production_db -t -A -F"," -c "{query}"\'],\n    capture_output=True,\n    text=True\n)\n\nlines = result.stdout.strip().split(\'\\n\')\n\nwith open(\'/backup/top_users.csv\', \'w\', newline=\'\') as f:\n    writer = csv.writer(f)\n    writer.writerow([\'username\', \'total_amount\'])\n    for line in lines:\n        if line.strip():\n            parts = line.split(\',\')\n            if len(parts) == 2:\n                writer.writerow([parts[0], parts[1]])\n\nprint(f\'Generated top_users.csv with {len(lines)} rows\')\nEOF\n'
'python3 /backup/generate_top_users.py\n'
'cat /backup/top_users.csv\n'
'wc -l /backup/top_users.csv\n'
"ls -la /backup/*.txt /backup/*.csv /backup/*.json | grep -E '(clean_users|valid_transactions|db_stats|top_users|backup_verification|recovery_complete)'\n"
'cat /backup/recovery_complete.txt\n'
'tail -1 /backup/clean_users.csv | od -c | head -2\n'
'tail -1 /backup/valid_transactions.json | od -c | head -2\n'
'tail -1 /backup/db_stats.txt | od -c | head -2\n'
'tail -1 /backup/top_users.csv | od -c | head -2\n'
'tail -1 /backup/backup_verification.txt | od -c | head -2\n'
'tail -1 /backup/recovery_complete.txt | od -c | head -2\n'
'tail -1 /backup/clean_users.csv\n'
'python3 -c "with open(\'/backup/clean_users.csv\', \'a\') as f: f.write(\'\\n\')" 2>/dev/null || echo \'\'\n'
'tail -1 /backup/clean_users.csv | od -c | head -2\n'
'python3 -c "content = open(\'/backup/top_users.csv\', \'rb\').read(); open(\'/backup/top_users.csv\', \'wb\').write(content.replace(b\'\\r\\n\', b\'\\n\'))"\n'
'tail -1 /backup/top_users.csv | od -c | head -2\n'
'su - postgres -c "psql -d production_db -c \'SELECT COUNT(*) as user_count FROM users; SELECT COUNT(*) as txn_count FROM transactions;\'"\n'
"echo '=== File Verification ===' && ls -lh /backup/{clean_users.csv,valid_transactions.json,db_stats.txt,top_users.csv,backup_verification.txt,recovery_complete.txt}\n"
"echo '=== clean_users.csv sample ===' && head -3 /backup/clean_users.csv && echo '...' && tail -2 /backup/clean_users.csv\n"
"echo '=== valid_transactions.json sample ===' && head -10 /backup/valid_transactions.json && echo '...'\n"
"echo '=== All reports ===' && cat /backup/db_stats.txt && echo '' && cat /backup/backup_verification.txt && echo '' && cat /backup/recovery_complete.txt\n"
'wc -l /backup/clean_users.csv /backup/top_users.csv\n'
'python3 -c "import json; data=json.load(open(\'/backup/valid_transactions.json\')); print(f\'Valid transactions: {len(data)}\'); print(f\'Sample: {data[0]}\')"\n'
"echo '=== Task Complete - All Requirements Met ===' && echo 'Database: production_db created and populated' && echo 'Users table: 30 rows' && echo 'Transactions table: 100 rows' && echo 'All output files generated and properly formatted'\n"
['asciinema rec --stdin /logs/tests.cast', 'Enter']
['clear', 'Enter']
['bash ', '/tests/run-tests.sh', 'Enter']
['C-d']
['C-d']
